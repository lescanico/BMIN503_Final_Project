---
title: "A Data-Driven Approach to Predicting and Optimizing Outpatient Psychiatry Resource Utilization"
subtitle: "BMIN503/EPID600 Final Project"
author: "Nicolas Lescano"
output: html_document
format:
  html:
    css: "style.css"
    toc: true
    toc-depth: 4
    toc-location: left
    toc_float:
      smooth_scroll: true
editor: visual
number-sections: true
embed-resources: true
execute:
  message: false
  warning: false
---

![](images/banner.jpg){fig-align="left"}

------------------------------------------------------------------------

## Overview {#sec-overview}

This project leverages a decade of historical data from the Penn Behavioral Health Outpatient Psychiatry Clinic (PBH OPC) to develop predictive models aimed at optimizing resource utilization. The goal is to address operational inefficiencies in scheduling and care allocation, ultimately enhancing patient outcomes, clinic efficiency, and provider satisfaction. Integrating principles from psychiatry, healthcare operations, and data science, the project offers actionable insights into improving outpatient mental health care delivery.

The complete project repository, including scripts and datasets, is available [here](https://github.com/lescanico/BMIN503_Final_Project).

## Introduction {#sec-introduction}

Outpatient psychiatry clinics often face significant challenges in managing the balance between patient demand and resource availability. Inefficiencies, such as long wait times, underutilized provider hours, and mismatches in the level of care, negatively impact patient outcomes and clinic operations. For instance, delayed follow-ups and uncoordinated care transitions contribute to treatment disruptions and patient dissatisfaction.

This project addresses these challenges by applying a data-driven approach to predict resource needs and inform scheduling decisions. Predictive models enable more accurate forecasting of appointment frequency, duration, and type of provider required, reducing operational inefficiencies and supporting proactive resource planning.

## Methods {#sec-methods}

### **Data Sourcing**

Patient and visit data spanning 10/1/2014 to 9/30/2024 were extracted from Epic Analytics. The raw files, originally in `.xlsx` format, were converted to `.csv` for analysis and securely stored as `patient_data.csv` and `visit_data.csv`.

### Data Anonymization

Sensitive identifiers such as medical record numbers (MRNs) were replaced with anonymized IDs. Key demographics like birth dates and postal codes were generalized to ensure compliance with HIPAA and other privacy regulations.

```{r anonymization, eval=FALSE}
# Load required libraries
library(readr)
library(dplyr)
library(lubridate)

# Import raw data
patient_data_raw <- read_csv("H:/secure/patient_data.csv")
visit_data_raw <- read_csv("H:/secure/visit_data.csv")

# Generate unique patient IDs for anonymization
mrn_lookup <- tibble(
  MRN = unique(patient_data_raw$MRN),
  patient_id = sprintf("%05d", seq_along(unique(patient_data_raw$MRN)))
)

# Save mrn_lookup for potential reversibility
saveRDS(mrn_lookup, file = "H:/secure/mrn_lookup.rds")

# Anonymize data by replacing sensitive identifiers
anonymize <- function(data, lookup) {
  data %>%
    left_join(lookup, by = "MRN") %>%
    mutate(
      year_of_birth = year(mdy(`Birth Date (UTC)`)),
      postal_code = substr(`Postal Code`, 1, 3)
    ) %>%
    select(-MRN, -`Birth Date (UTC)`, -`Postal Code`)
}

# Apply anonymization and save
patient_data_anonymized <- anonymize(patient_data_raw, mrn_lookup)
visit_data_anonymized <- anonymize(visit_data_raw, mrn_lookup)
saveRDS(patient_data_anonymized, "datasets/patient_data_anonymized.rds")
saveRDS(visit_data_anonymized, "datasets/visit_data_anonymized.rds")
```

### Data Preprocessing

#### Standardization and Deduplication:

Variable names were standardized (e.g., lowercase with underscores) for consistency. Duplicates arising from merged datasets were consolidated, with non-identical columns renamed using descriptive suffixes.

```{r renaming, eval=FALSE}
# Load required library
library(stringr)

# Inspect column names
colnames(patient_data_anonymized)
colnames(visit_data_anonymized)

# Define function to standardize column names
standardize_column_names <- function(dataset) {
  colnames(dataset) <- colnames(dataset) %>%
    str_to_lower() %>%
    str_replace_all("[\\s\\.\\/\\?\\-\\(\\)\\%\\$]+", "_") %>%
    str_replace_all("_+", "_") %>%
    str_replace_all("_$", "")
  dataset
}

# Apply standardization
patient_data_renamed <- standardize_column_names(patient_data_anonymized)
visit_data_renamed <- standardize_column_names(visit_data_anonymized)

# Remove unnecessary columns created during Epic export process
patient_data_renamed <- patient_data_renamed %>% select(-start_date, -end_date)
visit_data_renamed <- visit_data_renamed %>% select(-start_date, -end_date)

# Rename specific columns for consistency
colnames(patient_data_renamed)[colnames(patient_data_renamed) == "bp_diastolic_mmhg"] <- "bp_diastolic"
colnames(patient_data_renamed)[colnames(patient_data_renamed) == "bp_systolic_mmhg"] <- "bp_systolic"
colnames(visit_data_renamed)[colnames(visit_data_renamed) == "bmi_kg_m^2"] <- "bmi"

# Add suffixes to duplicate columns, excluding "patient_id"
common_cols <- intersect(colnames(patient_data_renamed), colnames(visit_data_renamed))
common_cols <- setdiff(common_cols, "patient_id")

# Add suffixes to common columns in both datasets
colnames(patient_data_renamed)[colnames(patient_data_renamed) %in% common_cols] <- paste0(common_cols, "_from_patient_dataset")
colnames(visit_data_renamed)[colnames(visit_data_renamed) %in% common_cols] <- paste0(common_cols, "_from_visit_dataset")

# Visualize final column names
cat("Columns in patient_data_renamed:\n")
print(colnames(patient_data_renamed))

cat("\nColumns in visit_data_renamed:\n")
print(colnames(visit_data_renamed))
```

###### Patient Dataset Column Handling Table

<iframe src="figures/patient_column_handling.html">

</iframe>

###### Visit Dataset Column Handling Table

<iframe src="figures/visit_column_handling.html">

</iframe>

##### Handling Variable Types

Data types were converted to ensure compatibility with analytical processes. Dates, categorical variables, and numerical data were reformatted for accurate statistical analysis and modeling.

```{r conversions, eval=FALSE}
# Load required libraries
library(tidyr)
library(purrr)

# Helper functions for data type conversion
convert_to_date <- function(col) {
  as.Date(parse_date_time(col, orders = c("dmy", "mdy", "ymd"), quiet = TRUE))
}

convert_to_posix <- function(col) {
  if (is.numeric(col)) as.POSIXct(col, origin = "1970-01-01") else col
}

convert_to_list <- function(col) {
  if (is.character(col)) {
    str_split(col, pattern = "[,;\n|]+")
  } else {
    col
  }
}

safe_numeric <- function(col) {
  if (is.list(col)) {
    as.numeric(unlist(col))
  } else {
    suppressWarnings(as.numeric(col))
  }
}

# Function to convert data types and summarize changes
convert_data_types <- function(dataset, conversions, dataset_name) {
  initial_types <- map_chr(dataset, ~ paste(class(.x), collapse = ", "))  # Track initial data types
  
  # Apply type conversions
  dataset <- dataset %>%
    mutate(
      across(any_of(conversions$Date), convert_to_date),
      across(any_of(conversions$POSIX), convert_to_posix),
      across(any_of(conversions$Factor), as.factor),
      across(any_of(conversions$Numeric), safe_numeric),  # Handle conversion warnings
      across(any_of(conversions$Logical), as.logical),
      across(any_of(conversions$List), convert_to_list)
    )
  
  # Capture and display changes from initial to final types
  final_types <- map_chr(dataset, ~ paste(class(.x), collapse = ", "))
  changes <- tibble(Column = names(dataset), Initial_Type = initial_types, Final_Type = final_types)
  
  # Identify columns with no changes
  converted <- changes %>% filter(Initial_Type != Final_Type)
  unchanged <- changes %>% filter(Initial_Type == Final_Type)
  unprocessed <- tibble(Column = setdiff(names(dataset), names(initial_types)), Initial_Type = "Unknown", Final_Type = "Unprocessed")
  
  cat("\nData Type Conversion Summary for ", dataset_name, ":\n", sep = "")
  
  # Converted columns
  if (nrow(converted) > 0) {
    cat("\nConverted Columns:\n")
    print(converted, n = Inf)
  } else {
    cat("\nNo columns were converted.\n")
  }
  
  # Unchanged columns
  if (nrow(unchanged) > 0) {
    cat("\nUnchanged Columns:\n")
    print(unchanged, n = Inf)
  }
  
  # Unprocessed columns
  if (nrow(unprocessed) > 0) {
    cat("\nUnprocessed Columns:\n")
    print(unprocessed, n = Inf)
  }
  
  return(dataset)
}

# Define type conversions for the patient dataset
patient_conversions <- list(
  Date = c(),
  Factor = c(
    "state", "marital_status", "gender_identity_from_patient_dataset", "sdoh_risk_level",
    "language_from_patient_dataset", "legal_sex_from_patient_dataset", "country_from_patient_dataset",
    "country_county_from_patient_dataset", "religion_from_patient_dataset",
    "sexual_orientation_from_patient_dataset", "sex_assigned_at_birth_from_patient_dataset",
    "level_of_service_from_patient_dataset", "mychart_status_from_patient_dataset",
    "patient_ethnic_group_from_patient_dataset", "patient_race_from_patient_dataset",
    "rural_urban_commuting_area_primary_from_patient_dataset", "rural_urban_commuting_area_secondary_from_patient_dataset",
    "patient_id_from_patient_dataset", "postal_code_from_patient_dataset", 
    "state_from_patient_dataset"
  ),
  Numeric = c(
    "adi_national_percentile", "adi_state_decile", "bmi_from_patient_dataset",
    "bp_diastolic_from_patient_dataset", "bp_systolic_from_patient_dataset", 
    "year_of_birth_from_patient_dataset", "svi_2020_socioeconomic_percentile_census_tract"
  ),
  Logical = c(
    "interpreter_needed_from_patient_dataset", "university_of_pennsylvania_student_from_patient_dataset"
  ),
  List = c(
    "allergies_and_contraindications", "chief_complaint", "hospital_or_clinic_administered_medications",
    "medical_history", "medications", "outpatient_medications", "procedures", "procedures_ordered_from_patient_dataset",
    "sdoh_domains", "phq_2_total_score", "phq_9", "medications_ordered_from_patient_dataset",
    "diagnosis_from_patient_dataset", "general_risk_score"
  )
)

# Define type conversions for the visit dataset
visit_conversions <- list(
  Date = c("visit_date", "appointment_creation_date"),
  POSIX = "appointment_time",
  Factor = c(
    "visit_type", "appointment_status", "encounter_type", "primary_benefit_plan",
    "primary_diagnosis", "primary_payer", "primary_payer_financial_class", 
    "primary_provider_title", "primary_provider_type", "scheduling_source", 
    "gender_identity_from_visit_dataset", "language_from_visit_dataset", 
    "legal_sex_from_visit_dataset", "religion_from_visit_dataset", 
    "sexual_orientation_from_visit_dataset", "state_from_visit_dataset", 
    "country_from_visit_dataset", "country_county_from_visit_dataset", 
    "primary_subscriber_group_number", "postal_code_from_visit_dataset",
    "patient_ethnic_group_from_visit_dataset", "patient_race_from_visit_dataset",
    "rural_urban_commuting_area_primary_from_visit_dataset", "rural_urban_commuting_area_secondary_from_visit_dataset",
    "patient_id_from_visit_dataset", 
    "mychart_status_from_visit_dataset",
    "sex_assigned_at_birth_from_visit_dataset", "level_of_service_from_visit_dataset"
  ),
  Numeric = c(
    "age_at_visit_years", "appointment_length_minutes", "lead_time_days", "continuity_of_care",
    "copay_collected", "copay_due", "prepayment_collected", "prepayment_due",
    "time_physician_spent_post_charting_minutes", "time_physician_spent_pre_charting_minutes",
    "time_waiting_for_physician_minutes", "time_with_physician_minutes", 
    "no_show_probability", "bmi_from_visit_dataset", "bp_diastolic_from_visit_dataset", 
    "bp_systolic_from_visit_dataset", "year_of_birth_from_visit_dataset", "encounter_to_close_day"
  ),
  Logical = c(
    "new_to_department_specialty", "new_to_facility", "new_to_provider", 
    "portal_active_at_scheduling", "self_pay", "university_of_pennsylvania_student_from_visit_dataset",
    "interpreter_needed_from_visit_dataset"
  ),
  List = c(
    "diagnosis_from_visit_dataset", "procedures_ordered_from_visit_dataset", "medications_ordered_from_visit_dataset"
  )
)

# Apply conversions to both datasets
patient_data_converted <- convert_data_types(patient_data_renamed, patient_conversions, "Patient Data")
visit_data_converted <- convert_data_types(visit_data_renamed, visit_conversions, "Visit Data")

# Create a single combined lookup dataframe for variable names and types
variable_type_lookup <- data.frame(
  Variable = c(names(patient_data_converted), names(visit_data_converted)),
  Type = c(
    map_chr(patient_data_converted, ~ paste(class(.), collapse = ", ")),
    map_chr(visit_data_converted, ~ paste(class(.), collapse = ", "))
  ),
  stringsAsFactors = FALSE
)

# Remove duplicates by keeping the first occurrence of each variable
variable_type_lookup <- variable_type_lookup[!duplicated(variable_type_lookup$Variable), ]

# Save the combined lookup dataframe as an .rds file
saveRDS(variable_type_lookup, file = "datasets/variable_type_lookup.rds")

# Print confirmation and preview the dataframe
cat("Variable type lookup dataframe saved as 'variable_type_lookup.rds'.\n")
print(head(variable_type_lookup))
```

###### Patient Dataset Type Conversion Table

<iframe src="figures/patient_type_conversions.html">

</iframe>

###### Visit Dataset Type Conversion Table

<iframe src="figures/visit_type_conversions.html">

</iframe>

#### Data Integration

The process involved merging patient and visit data based on unique identifiers, ensuring all related records were combined accurately.

##### Merging Datasets

Patient and visit data were merged using unique patient IDs. This process combined all related records into a unified dataset for comprehensive analysis. Any conflicts between overlapping columns were resolved by renaming non-identical columns with descriptive suffixes.

```{r merging, eval=FALSE}
# Verify unique patient_id values are shared
unique_patient_ids_patient_data <- unique(patient_data_converted$patient_id)
unique_patient_ids_visit_data <- unique(visit_data_converted$patient_id)

all_shared <- all(unique_patient_ids_patient_data %in% unique_patient_ids_visit_data) &&
              all(unique_patient_ids_visit_data %in% unique_patient_ids_patient_data)

if (all_shared) {
  print("All unique patient_id values are shared between the two datasets.")
} else {
  print("There are patient_id values that are not shared between the two datasets.")
}

# Perform the merge
merged_data <- visit_data_converted %>%
  left_join(patient_data_converted, by = "patient_id")

# Check consistency in dimensions
dim(patient_data_converted)
dim(visit_data_converted)
dim(merged_data)
```

##### Deduplication

Duplicate columns were identified and processed. Identical values were consolidated, while non-identical columns were renamed appropriately to retain critical information.

```{r deduplication, eval=FALSE}
# Function to compare duplicate columns, check if they are identical, and remove duplicates
process_duplicate_columns <- function(data, suffix_1 = "_from_patient_dataset", suffix_2 = "_from_visit_dataset") {
  # Extract column names ending with the specified suffixes
  cols_suffix_1 <- grep(paste0(suffix_1, "$"), names(data), value = TRUE)
  cols_suffix_2 <- grep(paste0(suffix_2, "$"), names(data), value = TRUE)
  
  # Initialize an empty data frame for storing results
  results <- data.frame(Column_1 = character(), Column_2 = character(), Identical = logical(), stringsAsFactors = FALSE)
  
  # Vector to store non-identical column names
  non_identical_columns <- c()
  
  # Loop through columns with the same prefix but different suffixes
  for (col in cols_suffix_1) {
    # Derive the corresponding column name with the other suffix
    corresponding_col <- gsub(suffix_1, suffix_2, col)
    
    # Check if the corresponding column exists
    if (corresponding_col %in% names(data)) {
      identical_check <- identical(data[[col]], data[[corresponding_col]])
      results <- rbind(
        results,
        data.frame(
          Column_1 = col,
          Column_2 = corresponding_col,
          Identical = identical_check,
          stringsAsFactors = FALSE
        )
      )
      
      if (identical_check) {
        # Remove the suffix from the kept column
        new_name <- gsub(paste0("_from_patient_dataset|_from_visit_dataset"), "", col)
        names(data)[names(data) == col] <- new_name
        # Drop the duplicate column
        data <- data %>% select(-all_of(corresponding_col))
      } else {
        # Store non-identical columns for further analysis
        non_identical_columns <- c(non_identical_columns, col, corresponding_col)
      }
    } else {
      results <- rbind(
        results,
        data.frame(
          Column_1 = col,
          Column_2 = NA,
          Identical = NA,
          stringsAsFactors = FALSE
        )
      )
    }
  }
  
  # Return the updated data, the comparison results, and non-identical columns
  list(
    deduplicated_data = data,
    duplicate_check_results = results,
    non_identical_columns = unique(non_identical_columns)
  )
}

# Apply the function
processed_results <- process_duplicate_columns(merged_data)

# Extract the deduplicated dataset, duplicate check results, and non-identical columns
deduplicated_data <- processed_results$deduplicated_data
duplicate_column_check <- processed_results$duplicate_check_results
non_identical_columns <- processed_results$non_identical_columns

# Display results
print(duplicate_column_check)
print("Identical Columns:")
nrow(duplicate_column_check[duplicate_column_check$Identical == TRUE, ])
print("Non-Identical Columns:")
nrow(duplicate_column_check[duplicate_column_check$Identical == FALSE, ])

# Function to rename non-identical duplicate columns
rename_non_identical_columns <- function(data) {
  # Define mappings for non-identical columns
  rename_mapping <- c(
    "bmi_from_patient_dataset" = "patient_bmi",
    "bmi_from_visit_dataset" = "visit_bmi",
    "bp_diastolic_from_patient_dataset" = "patient_bp_diastolic",
    "bp_diastolic_from_visit_dataset" = "visit_bp_diastolic",
    "bp_systolic_from_patient_dataset" = "patient_bp_systolic",
    "bp_systolic_from_visit_dataset" = "visit_bp_systolic",
    "country_county_from_patient_dataset" = "patient_county",
    "country_county_from_visit_dataset" = "visit_county",
    "diagnosis_from_patient_dataset" = "patient_diagnosis",
    "diagnosis_from_visit_dataset" = "visit_diagnosis",
    "gender_identity_from_patient_dataset" = "patient_gender_identity",
    "gender_identity_from_visit_dataset" = "visit_gender_identity",
    "level_of_service_from_patient_dataset" = "patient_service_level",
    "level_of_service_from_visit_dataset" = "visit_service_level",
    "medications_ordered_from_patient_dataset" = "patient_medications_ordered",
    "medications_ordered_from_visit_dataset" = "visit_medications_ordered",
    "patient_race_from_patient_dataset" = "patient_race",
    "patient_race_from_visit_dataset" = "visit_race",
    "procedures_ordered_from_patient_dataset" = "patient_procedures_ordered",
    "procedures_ordered_from_visit_dataset" = "visit_procedures_ordered",
    "religion_from_patient_dataset" = "patient_religion",
    "religion_from_visit_dataset" = "visit_religion",
    "sex_assigned_at_birth_from_patient_dataset" = "patient_sex_assigned",
    "sex_assigned_at_birth_from_visit_dataset" = "visit_sex_assigned",
    "sexual_orientation_from_patient_dataset" = "patient_sexual_orientation",
    "sexual_orientation_from_visit_dataset" = "visit_sexual_orientation",
    "postal_code_from_patient_dataset" = "patient_postal_code",
    "postal_code_from_visit_dataset" = "visit_postal_code"
  )
  
  # Rename columns in the dataset based on the mapping
  renamed_data <- data %>%
    rename_with(~ ifelse(.x %in% names(rename_mapping), rename_mapping[.x], .x))
  
  return(renamed_data)
}

# Apply the function to the deduplicated dataset
deduplicated_data_renamed <- rename_non_identical_columns(deduplicated_data)

# Verify renamed columns
print(names(deduplicated_data_renamed))
```

##### Reordering & Sorting

The merged dataset was reordered and sorted to prioritize relevant information. Columns were organized by their importance and relevance to the analysis objectives, and data was sorted primarily by patient ID and secondary by visit date. This organization facilitated easier access and manipulation of data for subsequent analyses.

```{r reordering-sorting, eval=FALSE}
# Reorder columns into logical groups
deduplicated_data_reordered <- deduplicated_data_renamed %>%
  select(
    # Identifiers
    patient_id,
    
    # Demographics
    age_at_visit_years, year_of_birth, patient_gender_identity, visit_gender_identity, patient_sex_assigned, visit_sex_assigned, language, interpreter_needed, legal_sex, marital_status, country, patient_county, visit_county, state, patient_postal_code, visit_postal_code,university_of_pennsylvania_student,
    
    # Visit Information
    visit_date, visit_type, appointment_creation_date, appointment_time, appointment_length_minutes, lead_time_days, continuity_of_care, portal_active_at_scheduling, new_to_department_specialty, new_to_facility, new_to_provider, encounter_type, scheduling_source,
    
    # Health Metrics
    patient_bmi, visit_bmi, patient_bp_diastolic, visit_bp_diastolic, patient_bp_systolic, visit_bp_systolic,
    
    # Symptoms, Diagnoses, and Procedures
    chief_complaint, primary_diagnosis, patient_diagnosis, visit_diagnosis, procedures, patient_procedures_ordered, visit_procedures_ordered,
    
    # Medications
    medications, outpatient_medications, patient_medications_ordered, visit_medications_ordered, hospital_or_clinic_administered_medications,
    
    # Service Level Information
    visit_service_level, patient_service_level,
    
    # Medical and Clinical History
    allergies_and_contraindications, medical_history,
    
    # Risk and Assessments
    general_risk_score, phq_2_total_score, phq_9, sdoh_domains, sdoh_risk_level,
    
    # Sociodemographic Context
    adi_national_percentile, adi_state_decile, rural_urban_commuting_area_primary, rural_urban_commuting_area_secondary, svi_2020_socioeconomic_percentile_census_tract,
    
    # Race, Ethnicity, Religion, and Sexual Orientation
    patient_race, visit_race, patient_religion, visit_religion, patient_ethnic_group, patient_sexual_orientation, visit_sexual_orientation,
    
    # Financial Information
    copay_collected, copay_due, prepayment_collected, prepayment_due, self_pay, primary_payer, primary_payer_financial_class, primary_subscriber_group_number, primary_benefit_plan,
    
    # Provider Information
    primary_provider_type, primary_provider_title,
    
    # Operational Metrics
    encounter_to_close_day, time_physician_spent_pre_charting_minutes, time_physician_spent_post_charting_minutes, time_waiting_for_physician_minutes, time_with_physician_minutes,
    
    # MyChart and Scheduling Metrics
    mychart_status, no_show_probability, appointment_status,

    # Remaining Columns (if any)
    everything()
  )

# Check reordered columns
print(names(deduplicated_data_reordered))

# Sort rows by patient_id and then by visit_date
data_preprocessed <- deduplicated_data_reordered %>%
  arrange(patient_id, visit_date)

# Verify resulting dataset
head(data_preprocessed)

# Identify variables in data_preprocessed that are not in the lookup table
new_variables <- setdiff(colnames(data_preprocessed), variable_type_lookup$Variable)

if (length(new_variables) > 0) {
  # Infer the types of the new variables
  inferred_types <- sapply(data_preprocessed[, new_variables, drop = FALSE], class)
  
  # Create a data frame for new variables and their inferred types
  new_rows <- data.frame(
    Variable = new_variables,
    Type = inferred_types,
    stringsAsFactors = FALSE
  )
  
  # Append the new variables with their types to the lookup table
  variable_type_lookup <- rbind(variable_type_lookup, new_rows)
  
  message(length(new_variables), " new variables with types added to variable_type_lookup.")
} else {
  message("No new variables to add.")
}

# Save the updated lookup table
saveRDS(variable_type_lookup, "datasets/variable_type_lookup.rds")

# Verify the update
print("Updated variable_type_lookup saved to 'datasets/variable_type_lookup.rds'")
```

### Missing Data Analysis

The analysis of missing data was performed to understand its extent and impact on the study. The process included visualizing missing data patterns and deciding on strategies to handle them. This was crucial to maintain the robustness of the statistical analysis and the reliability of the study results.

#### Missingness Visualization

Missing data patterns were analyzed to identify variables with significant gaps. Visualization techniques, such as bar plots, were used to summarize the extent of missingness.

```{r missing-plotting, eval=FALSE}
# Load required libraries
library(ggplot2)

# Define function to plot missing values
plot_missing_values_by_type <- function(dataset, min_missing, max_missing) {
  
  # Ensure column types are matched with the lookup dataframe
  variable_types <- setNames(variable_type_lookup$Type, variable_type_lookup$Variable)
  
  # Create missing data summary
  missing_summary <- dataset %>%
    summarise(across(everything(), ~ sum(is.na(.)) / n() * 100)) %>%
    pivot_longer(everything(), names_to = "column", values_to = "missing_percentage") %>%
    mutate(type = variable_types[column]) %>%
    filter(missing_percentage > min_missing & missing_percentage <= max_missing)

  # Plot if there is missing data
  if (nrow(missing_summary) > 0) {
    threshold_inside <- max_missing * 0.8
    ggplot(missing_summary, aes(x = reorder(column, -missing_percentage), y = missing_percentage, fill = type)) +
      geom_bar(stat = "identity", position = position_dodge2(width = 0.9)) +
      geom_text(data = missing_summary %>% filter(missing_percentage > 0), aes(
        label = ifelse(missing_percentage < 0.01, "<0.01", sprintf("%.2f", missing_percentage)),
        hjust = ifelse(missing_percentage > threshold_inside, 1.2, -0.2)
      ), size = 3, color = ifelse(missing_summary$missing_percentage > threshold_inside, "white", "black")) +
      coord_flip(clip = "off") +
      labs(
        x = "Column", y = "Missing Percentage"
      ) +
      scale_fill_manual(values = c(
        "Date" = "#1f77b4", 
        "POSIXct" = "#ff7f0e", 
        "hms" = "#17becf",
        "factor" = "#2ca02c",
        "numeric" = "#d62728", 
        "logical" = "#9467bd", 
        "list" = "#8c564b"
      )) +
      theme_minimal() +
      theme(
        legend.position = "top", legend.title = element_blank(), legend.direction = "horizontal",
        axis.text.y = element_text(size = 7, hjust = 1),
        plot.title = element_text(size = 14, face = "bold", hjust = 0),
        plot.margin = margin(20, 20, 20, 20)
      )
  } else {
    message("No columns with missing data in the specified range.")
  }
}

# Plot missing data levels
plot_missing_values_by_type(data_preprocessed, min_missing = 0, max_missing = 1)
plot_missing_values_by_type(data_preprocessed, min_missing = 1, max_missing = 5)
plot_missing_values_by_type(data_preprocessed, min_missing = 5, max_missing = 30)
plot_missing_values_by_type(data_preprocessed, min_missing = 30, max_missing = 50)
plot_missing_values_by_type(data_preprocessed, min_missing = 50, max_missing = 100)
```

##### Missing Data (0-1%) Plot

![](figures/0-1_missing_data_plot.png)

##### Missing Data (1-5%) Plot

![](figures/1-5_missing_data_plot.png)

##### Missing Data (5-30%) Plot

![](figures/5-30_missing_data_plot.png)

##### Missing Data (30-50%) Plot

![](figures/30-50_missing_data_plot.png)

##### Missing Data (50-100%) Plot

![](figures/50-100_missing_data_plot.png)

#### Handling Strategies

Variables were categorized into levels of missingness (e.g., Low: 0–5%, Moderate: 5–30%) to guide data handling strategies. Techniques included:

-   **Imputation**: Using mean, median, or predictive modeling to fill missing values.

-   **Exclusion**: Removing variables or records with excessive missing data.

##### Default Strategies

Approaches such as imputation (mean, median, or predictive), categorical substitution, or exclusion were applied based on the extent and type of missing data.

```{r default-strategies, eval=FALSE}
# Define missing data handling options
missing_data_options <- list(
  "Low (0-5%)" = list(
    Numeric = c(
      "Predictive Modeling (Regression)",
      "Predictive Modeling (KNN)",
      "Multiple Imputation (MICE)",
      "Mean Imputation",
      "Median Imputation",
      "Linear Interpolation",
      "Spline Interpolation",
      "Indicator Variable + Mean",
      "Indicator Variable + Median"
    ),
    Factor = c(
      "Predictive Modeling (Decision Tree)",
      "Predictive Modeling (Random Forest)",
      "Frequency-Based Imputation (Weighted by Subgroup Frequency)",
      "Mode Imputation",
      "Add 'Unknown' Category",
      "Add 'Other' Category"
    ),
    Logical = c(
      "Predictive Modeling (Logistic Regression)",
      "Mode Imputation",
      "Add Indicator for Missingness",
      "Replace with Most Frequent Value",
      "Assume FALSE (if reasonable)",
      "Assume TRUE (if reasonable)"
    ),
    Date = c(
      "Median Date Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Use Most Frequent Date",
      "Use Previous Valid Value",
      "Use Subsequent Valid Value"
    ),
    POSIX = c(
      "Median Timestamp Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Use Most Frequent Timestamp",
      "Use Previous Valid Timestamp",
      "Use Subsequent Valid Timestamp"
    ),
    List = c(
      "Predictive Modeling (List Similarity-Based)",
      "Weighted Average List Imputation",
      "Replace with Most Frequent List",
      "Replace with Proxy List",
      "Replace with Empty List",
      "Add Indicator for Missingness"
    )
  ),
  "Moderate (5-30%)" = list(
    Numeric = c(
      "Multiple Imputation (MICE)",
      "Predictive Imputation (Regression)",
      "Predictive Imputation (KNN)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Mean Imputation",
      "Median Imputation",
      "Indicator Variable + Mean",
      "Indicator Variable + Median"
    ),
    Factor = c(
      "Predictive Modeling (Random Forest)",
      "Predictive Modeling (Decision Tree)",
      "Frequency-Based Imputation (Weighted by Subgroup Frequency)",
      "Add 'Unknown' Category",
      "Add 'Other' Category",
      "Mode Imputation"
    ),
    Logical = c(
      "Predictive Imputation (Logistic Regression)",
      "Add Indicator for Missingness",
      "Mode Imputation",
      "Replace with Most Frequent Value",
      "Assume FALSE",
      "Assume TRUE"
    ),
    Date = c(
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Indicator + Median Date Imputation",
      "Use Previous Valid Value",
      "Use Subsequent Valid Value"
    ),
    POSIX = c(
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Indicator + Median Timestamp Imputation",
      "Use Previous Valid Timestamp",
      "Use Subsequent Valid Timestamp"
    ),
    List = c(
      "Predictive Modeling (Clustering-Based List Imputation)",
      "Weighted Average List Imputation",
      "Replace with Most Frequent List",
      "Replace with Proxy List",
      "Replace with Empty List",
      "Create Synthetic List"
    )
  ),
  "High (30-50%)" = list(
    Numeric = c(
      "Indicator Variable + Multiple Imputation (MICE)",
      "Indicator Variable + Predictive Modeling (Regression)",
      "Indicator Variable + Predictive Modeling (KNN)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Mean Imputation",
      "Median Imputation"
    ),
    Factor = c(
      "Predictive Modeling (Random Forest)",
      "Predictive Modeling (Decision Tree)",
      "Add 'Unknown' Category",
      "Add 'Other' Category",
      "Frequency-Based Imputation (Weighted by Subgroup Frequency)",
      "Use Most Frequent Category"
    ),
    Logical = c(
      "Indicator Variable + Predictive Imputation",
      "Add Indicator for Missingness",
      "Mode Imputation",
      "Replace with Most Frequent Value",
      "Assume FALSE",
      "Assume TRUE"
    ),
    Date = c(
      "Indicator Variable + Median Date Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Use Placeholder Dates"
    ),
    POSIX = c(
      "Indicator Variable + Median Timestamp Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Use Placeholder Timestamps"
    ),
    List = c(
      "Add Indicator Variable for Missingness",
      "Replace with Most Frequent List",
      "Replace with Proxy List",
      "Replace with Empty List",
      "Create Synthetic List (via Sampling or Clustering)"
    )
  ),
  "Very High (>50%)" = list(
    Numeric = c(
      "Indicator Variable + Rough Imputation (e.g., Overall Mean)",
      "Indicator Variable + Multiple Imputation (MICE)",
      "Add 'Unknown' Category",
      "Drop Variable (if non-critical)"
    ),
    Factor = c(
      "Add 'Unknown' Category",
      "Add 'Other' Category",
      "Use Most Frequent Category",
      "Frequency-Based Imputation (Weighted by Subgroup Frequency)",
      "Drop Variable (if non-critical)"
    ),
    Logical = c(
      "Indicator Variable + Assume FALSE",
      "Replace with Most Frequent Value",
      "Assume TRUE",
      "Drop Variable (if non-critical)"
    ),
    Date = c(
      "Use Placeholder Dates",
      "Indicator Variable + Rough Date Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Drop Variable (if non-critical)"
    ),
    POSIX = c(
      "Use Placeholder Timestamps",
      "Indicator Variable + Rough Timestamp Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Drop Variable (if non-critical)"
    ),
    List = c(
      "Add Indicator for Missingness",
      "Replace with Empty List",
      "Replace with Proxy List",
      "Create Synthetic List",
      "Drop Variable (if non-critical)"
    )
  )
)
```

##### Strategy Selection

The function `generate_data_handling_table` was designed to create a table for handling missing data and was utilized based on user input. Strategies for handling missing data were selected through three modes: manual selection for each variable, printing of all handling options, or random selection for testing purposes. This function assessed the type and extent of missingness, provided a summary of statistics, and facilitated the application of appropriate handling strategies. The results were presented in a styled table, categorized by the level of missingness and type of data, which aided in making informed decisions on how to address missing data in the dataset.

```{r handling-strategy-generator, eval=FALSE}
# Load required libraries
library(knitr)
library(kableExtra)
library(moments)

# Helper function to calculate summary statistics
calculate_summary_stats <- function(variable_data, type) {
  if (all(is.na(variable_data))) {
    return(list(description = "All values are missing"))
  }
  
  if (type == "Numeric") {
    stats <- summary(variable_data)
    sd_value <- sd(variable_data, na.rm = TRUE)
    skewness_value <- moments::skewness(variable_data, na.rm = TRUE)
    kurtosis_value <- moments::kurtosis(variable_data, na.rm = TRUE)
    range_values <- range(variable_data, na.rm = TRUE)
    
    description <- sprintf(
      "Range: %.1f to %.1f\n Median: %.1f\n Mean: %.1f\n SD: %.2f\n Skewness: %.2f\n Kurtosis: %.2f",
      range_values[1], range_values[2], stats["Median"], stats["Mean"], 
      sd_value, skewness_value, kurtosis_value
    )
    
  } else if (type == "Factor") {
    levels_count <- nlevels(variable_data)
    freq_table <- sort(table(variable_data), decreasing = TRUE)
    most_common <- names(freq_table[1])
    most_common_count <- freq_table[1]
    missing_levels <- sum(is.na(variable_data))
    
    description <- sprintf(
      "Levels: %d\n Most Common: %s (%d occurrences)\n Missing Levels: %d",
      levels_count, most_common, most_common_count, missing_levels
    )
    
  } else if (type == "Logical") {
    true_percent <- mean(variable_data, na.rm = TRUE) * 100
    false_percent <- 100 - true_percent
    
    description <- sprintf(
      "TRUE: %.1f%%\n FALSE: %.1f%%",
      true_percent, false_percent
    )
    
  } else if (type == "Date" || type == "POSIX") {
    range_values <- range(variable_data, na.rm = TRUE)
    missing_count <- sum(is.na(variable_data))
    
    description <- sprintf(
      "Range: %s to %s\n Missing: %d",
      format(range_values[1]), format(range_values[2]), missing_count
    )
    
  } else if (type == "List" && is.list(variable_data)) {
    lengths <- sapply(variable_data, length)
    range_lengths <- range(lengths, na.rm = TRUE)
    median_length <- median(lengths, na.rm = TRUE)
    mean_length <- mean(lengths, na.rm = TRUE)
    
    description <- sprintf(
      "Range Lengths: %d to %d\n Median Length: %.1f\n Mean Length: %.1f",
      range_lengths[1], range_lengths[2], median_length, mean_length
    )
    
  } else if (type == "Character") {
    unique_values <- length(unique(variable_data[!is.na(variable_data)]))
    most_common <- names(sort(table(variable_data), decreasing = TRUE))[1]
    missing_count <- sum(is.na(variable_data))
    
    description <- sprintf(
      "Unique Values: %d\n Most Common: %s\n Missing: %d",
      unique_values, most_common, missing_count
    )
    
  } else {
    description <- "Unsupported type"
  }
  
  # Add two trailing spaces to each line for Markdown rendering in outputs
  description <- gsub("\n", "  \n", description)
  
  return(list(description = description))
}

# Main function to generate missing data handling table
generate_data_handling_table <- function(dataset) {
  # Use variable type lookup dataframe
  variable_types <- setNames(variable_type_lookup$Type, variable_type_lookup$Variable)
  
  cat("Select function mode:\n")
  cat("1: Manually select handling strategy for each variable\n")
  cat("2: Print all handling strategy options for each variable\n")
  cat("3: Randomly select handling strategy for each variable (function testing)\n")
  
  repeat {
    selected_mode <- as.integer(readline("Enter your choice (1, 2, or 3): "))
    if (selected_mode %in% c(1, 2, 3)) break
    cat("Invalid choice. Try again.\n")
  }
  
  # Create the missing data summary
  missing_data_summary <- dataset %>%
    summarise(across(everything(), ~ sum(is.na(.)) / n() * 100)) %>%
    pivot_longer(everything(), names_to = "variable", values_to = "missing_percentage") %>%
    filter(missing_percentage > 0) %>%
    rowwise() %>%
    mutate(
      type = case_when(
        grepl("numeric|int|double|float", variable_types[variable], ignore.case = TRUE) ~ "Numeric",
        grepl("list|array|vector", variable_types[variable], ignore.case = TRUE) ~ "List",
        grepl("factor|categorical", variable_types[variable], ignore.case = TRUE) ~ "Factor",
        grepl("logical|bool|binary", variable_types[variable], ignore.case = TRUE) ~ "Logical",
        grepl("character|string|text", variable_types[variable], ignore.case = TRUE) ~ "Character",
        grepl("date|time|year|day", variable_types[variable], ignore.case = TRUE) ~ "Date",
        grepl("hms|difftime|posix|timestamp|datetime", variable_types[variable], ignore.case = TRUE) ~ "POSIX",
        TRUE ~ "Unsupported"  # Catch-all for unrecognized types
      ),
      summary_stats = calculate_summary_stats(dataset[[variable]], type)$description,
      missingness_level = case_when(
        missing_percentage <= 5 ~ "Low (0-5%)",
        missing_percentage > 5 & missing_percentage <= 30 ~ "Moderate (5-30%)",
        missing_percentage > 30 & missing_percentage <= 50 ~ "High (30-50%)",
        missing_percentage > 50 ~ "Very High (>50%)"
      )
    ) %>%
    ungroup()
  
  # Create a separate column for HTML rendering
  missing_data_summary <- missing_data_summary %>%
    mutate(summary_stats_html = gsub("\n", "<br>", summary_stats))
  
  # Initialize handling_strategy column
  missing_data_summary$handling_strategy <- NA
  
  # Assign handling strategies based on function mode
  
  # Manual Mode
  if (selected_mode == 1) {
    handling_strategies <- vector("character", nrow(missing_data_summary))
    for (i in seq_len(nrow(missing_data_summary))) {
      cat("\nVariable:", missing_data_summary$variable[i], "\n")
      cat("Missingness:", 
          ifelse(missing_data_summary$missing_percentage[i] < 0.01, "<0.01%", 
                 sprintf("%.2f%%", missing_data_summary$missing_percentage[i])), "\n")
      cat("Type:", missing_data_summary$type[i], "\n")
      cat("Summary Stats\n", missing_data_summary$summary_stats[i], "\n\n")
      
      options_key <- missing_data_summary$missingness_level[i]
      type_key <- missing_data_summary$type[i]
      
      # Retrieve options
      options <- tryCatch(
        missing_data_options[[options_key]][[type_key]],
        error = function(e) { NULL }
      )
      
      if (is.null(options)) {
        cat("No valid options found for", options_key, "-", type_key, "\n")
        handling_strategies[i] <- "No valid options available"
      } else {
        cat("Options:\n")
        for (j in seq_along(options)) {
          cat(j, ": ", options[j], "\n", sep = "")
        }
        choice <- readline("Choose an option (enter the number) or type custom strategy: ")
        if (choice %in% as.character(seq_along(options))) {
          handling_strategies[i] <- options[as.integer(choice)]
        } else {
          handling_strategies[i] <- choice
        }
      }
    }
    missing_data_summary$handling_strategy <- handling_strategies
  }

  # All Options Mode
  else if (selected_mode == 2) {
    missing_data_summary <- missing_data_summary %>%
      rowwise() %>%
      mutate(
        handling_strategy = if (!is.null(missing_data_options[[missingness_level]][[type]])) {
          paste(
            sprintf("%d: %s", seq_along(missing_data_options[[missingness_level]][[type]]), 
                    missing_data_options[[missingness_level]][[type]]), 
            collapse = "<br>"
          )
        } else {
          "No valid options available"
        }
      ) %>%
      ungroup()
  }

  # Random Mode
  else if (selected_mode == 3) {
    missing_data_summary <- missing_data_summary %>%
      rowwise() %>%
      mutate(
        handling_strategy = if (!is.null(missing_data_options[[missingness_level]][[type]])) {
          sample(missing_data_options[[missingness_level]][[type]], 1)
        } else {
          "No valid options available"
        }
      ) %>%
      ungroup()
  }
  
  # Format and sort missing percentages
  missing_data_summary <- missing_data_summary %>%
    mutate(
      missing_percentage_display = ifelse(
        missing_percentage < 0.1, "<0.1%", sprintf("%.1f%%", missing_percentage)
      ),
      missing_sort_order = ifelse(missing_percentage < 0.1, -1, missing_percentage)
    ) %>%
    arrange(missing_sort_order, missingness_level, type, missing_percentage) %>%
    select(-missing_sort_order)  # Remove the helper column after sorting
  
  # Define colors for missingness levels
  colors <- c(
    "Low (0-5%)" = "#E8F8F5",
    "Moderate (5-30%)" = "#D6EAF8",
    "High (30-50%)" = "#FADBD8",
    "Very High (>50%)" = "#F5B7B1"
  )
  
  # Create a styled table with summary stats and handling strategies
  table <- missing_data_summary %>%
    select(
      `Variables by level of missingness` = variable,
      `Missing %` = missing_percentage_display,
      `Summary Stats` = summary_stats_html,  # Use HTML-formatted column
      `Handling Strategy` = handling_strategy
    ) %>%
    kbl(align = c("r", "c", "l", "l"), escape = FALSE) %>%
    kable_styling(full_width = FALSE, position = "center", font_size = 12) %>%
    column_spec(2, bold = TRUE, color = "blue") %>%
    column_spec(3, italic = TRUE) %>%
    column_spec(4, italic = TRUE) %>%
    row_spec(0, bold = TRUE, background = "#2E86C1", color = "white", font_size = 14)
  
  # Add row grouping by missingness level and apply colors
  unique_levels <- unique(missing_data_summary$missingness_level)
  for (level in unique_levels) {
    level_rows <- which(missing_data_summary$missingness_level == level)
    table <- table %>%
      group_rows(
        level,                           # Group label
        min(level_rows),                 # Start row
        max(level_rows),                 # End row
        label_row_css = paste0("background-color:", colors[[level]], ";")
      )
  }
  
  return(table)
}

# Generate table
missing_data_handling_table <- generate_data_handling_table(data_preprocessed)
```

###### Missing Data Handling Table with All Options

<iframe src="figures/missing_data_handling_table.html">

</iframe>

###### Manually Generated Missing Data Handling Table

<iframe src="figures/missing_data_handling_table_manual.html">

</iframe>

### Data Reduction

A reduced dataset with fewer variables will be created to meet final project deadline. Later iterations will explore the complete dataset.

### Exploratory Data Analysis (EDA)

Exploratory Data Analysis will be conducted to gain insights into the main characteristics of the data, focusing on the distribution of key variables and the relationships between them. This will involve visualizing data distributions, identifying outliers, and exploring potential groupings and patterns. The findings from EDA will help in understanding the underlying structure of the data and guide the subsequent phases of data preprocessing and model building.

### Data Transformation

Data will be transformed to fit the assumptions required for effective model training. This includes normalizing and scaling continuous variables to prevent attributes with larger ranges from dominating the model's feature importance. Categorical variables will be encoded into numerical values to facilitate their use in machine learning algorithms. Transformations such as logarithmic or square root transformations will be applied to skewed data to approximate normal distributions.

#### **Normalization and Scaling**

Numerical variables will be normalized using techniques such as Min-Max scaling or Z-score normalization to ensure equal contribution to models.

#### **Encoding**

Categorical variables will be converted into numerical representations using one-hot encoding or label encoding, depending on their nature.

### Feature Engineering

New features will be engineered to capture resource utilization effectively:

-   **Visit Count:** Total number of visits per patient.

-   **Visit Span:** Time span between a patient’s first and last recorded visits.

-   **Visit Density:** Number of visits divided by the duration of engagement.

-   **Diagnostic Complexity:** Count of unique diagnoses over time.

-   **Therapeutic Complexity:** Count of unique medications and procedures over time.

-   **Resource Utilization Score (RUS):** A composite metric combining visit density, diagnosis complexity, and medication burden to categorize patients into low, medium, or high utilization levels.

These features will be designed and normalized to improve model performance and enhance the interpretability of results.

#### Resource Utilization Score (RUS)

A Resource Utilization Score will be constructed using a composite index that integrates various dimensions of healthcare use, such as visit frequency, diversity of diagnoses, and medication load. This score will categorize patients into different levels of healthcare utilization, aiding in resource planning and management.

The RUS categorized patients into low, medium, or high utilization groups, providing insights into healthcare resource needs.

### Predictive Modeling

Predictive models will be developed to forecast healthcare resource utilization based on historical patient data. These models will help anticipate patient needs, optimizing resource allocation and scheduling. Both regression and classification models will be explored to predict continuous outcomes, such as the number of future visits, and categorical outcomes, such as high or low resource utilization.

#### Baseline Models

Baseline predictors, such as mean and median-based imputation, will serve as benchmarks for evaluating the performance of advanced algorithms.

#### Advanced Models

Both regression and classification models will be developed to predict resource utilization:

1.  **Regression Models:**

    -   **Linear Regression:** To obtain baseline performance for predicting continuous variables, such as visit frequency and duration.

    -   **Random Forest Regression:** To identify key predictors and achieve higher accuracy.

2.  **Classification Models:**

    -   **Logistic Regression:** Will categorize patients into low, medium, or high utilization groups.

    -   **Support Vector Machines (SVM):** To improve separation of utilization categories with complex patterns.

#### Evaluation Metrics

Model performance will be assessed using task-specific metrics:

-   **Regression:** Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared.

-   **Classification:** Accuracy, Precision, Recall, F1-score, and Area Under the Receiver Operating Characteristic Curve (AUC-ROC).

#### Validation and Optimization

To ensure generalizability, cross-validation techniques (e.g., k-fold cross-validation) will be employed. Grid search and random search methods will optimize hyperparameters for the best-performing models.

## Results

The results section will include:

1.  **Summary Statistics**

2.  **Model Performance**: Summary metrics highlighting predictive accuracy and reliability.

3.  **Feature Importance**: Insights into the most impactful predictors of resource utilization.

4.  **Validation**: Evaluation of model performance on unseen data.

## Conclusion {#sec-conclusion}
