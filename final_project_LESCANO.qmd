---
title: "A Data-Driven Approach to Predicting and Optimizing Outpatient Psychiatry Resource Utilization"
subtitle: "BMIN503/EPID600 Final Project"
author: "Nicolas Lescano"
editor: visual
format:
  html:
    css: "style.css"
    embed-resources: true
    self-contained: true
    toc: true
    toc-depth: 5
    toc-location: left
    code-fold: true
    code-fold-default: true
    code-tools: true
execute:
  message: false
  warning: false
---

![](images/banner.jpg){fig-align="left"}

------------------------------------------------------------------------

## Overview {#sec-overview}

This project leverages a decade of historical data from the Penn Behavioral Health Outpatient Psychiatry Clinic (PBH OPC) to develop predictive models aimed at optimizing resource utilization. The goal is to address operational inefficiencies in scheduling and care allocation, ultimately enhancing patient outcomes, clinic efficiency, and provider satisfaction. Integrating principles from psychiatry, healthcare operations, and data science, the project offers actionable insights into improving outpatient mental health care delivery.

The complete project repository, including scripts and datasets, is available [here](https://github.com/lescanico/BMIN503_Final_Project).

## Introduction {#sec-introduction}

Outpatient psychiatry clinics often face significant challenges in managing the balance between patient demand and resource availability. Inefficiencies, such as long wait times, underutilized provider hours, and mismatches in the level of care, negatively impact patient outcomes and clinic operations. For instance, delayed follow-ups and uncoordinated care transitions contribute to treatment disruptions and patient dissatisfaction.

This project addresses these challenges by applying a data-driven approach to predict resource needs and inform scheduling decisions. Predictive models enable more accurate forecasting of appointment frequency, duration, and type of provider required, reducing operational inefficiencies and supporting proactive resource planning.

## Methods {#sec-methods}

### **Data Sourcing**

Patient and visit data spanning 10/1/2014 to 9/30/2024 were extracted from Epic Analytics platform. The raw files, originally in `.xlsx` format, were converted to `.csv` and securely stored as `patient_data.csv` and `visit_data.csv`.

### Data Anonymization

Sensitive identifiers such as medical record numbers (MRNs) were replaced with anonymized IDs. Key demographics like birth dates and postal codes were generalized to ensure compliance with HIPAA and other privacy regulations.

```{r anonymization, eval=FALSE}
# Load required libraries
library(readr)
library(dplyr)
library(lubridate)

# Import raw data
patient_data_raw <- read_csv("H:/secure/patient_data.csv")
visit_data_raw <- read_csv("H:/secure/visit_data.csv")

# Generate unique patient IDs for anonymization
mrn_lookup <- tibble(
  MRN = unique(patient_data_raw$MRN),
  patient_id = sprintf("%05d", seq_along(unique(patient_data_raw$MRN)))
)

# Save mrn_lookup for potential reversibility
saveRDS(mrn_lookup, file = "H:/secure/mrn_lookup.rds")

# Anonymize data by replacing sensitive identifiers
anonymize <- function(data, lookup) {
  data %>%
    left_join(lookup, by = "MRN") %>%
    mutate(
      year_of_birth = year(mdy(`Birth Date (UTC)`)),
      postal_code = substr(`Postal Code`, 1, 3)
    ) %>%
    select(-MRN, -`Birth Date (UTC)`, -`Postal Code`)
}

# Apply anonymization and save
patient_data_anonymized <- anonymize(patient_data_raw, mrn_lookup)
visit_data_anonymized <- anonymize(visit_data_raw, mrn_lookup)
saveRDS(patient_data_anonymized, "datasets/patient_data_anonymized.rds")
saveRDS(visit_data_anonymized, "datasets/visit_data_anonymized.rds")
```

### Data Preprocessing

#### Name Standarization

Variable names were standardized (e.g., lowercase with underscores) for consistency. Duplicates arising from merged datasets were consolidated, with non-identical columns renamed using descriptive suffixes.

```{r renaming, eval=FALSE}
# Load required libraries
library(readr)
library(dplyr)
library(lubridate)
library(stringr)

# Load anonymized data
patient_data_anonymized <- readRDS("datasets/patient_data_anonymized.rds")
visit_data_anonymized <- readRDS("datasets/visit_data_anonymized.rds")

# Define function to standardize column names
standardize_column_names <- function(dataset) {
  colnames(dataset) <- colnames(dataset) %>%
    str_to_lower() %>%
    str_replace_all("[\\s\\.\\/\\?\\-\\(\\)\\%\\$]+", "_") %>%
    str_replace_all("_+", "_") %>%
    str_replace_all("_$", "")
  dataset
}

# Apply standardization
patient_data_renamed <- standardize_column_names(patient_data_anonymized)
visit_data_renamed <- standardize_column_names(visit_data_anonymized)

# Remove unnecessary columns created during Epic export process
patient_data_renamed <- patient_data_renamed %>% select(-start_date, -end_date)
visit_data_renamed <- visit_data_renamed %>% select(-start_date, -end_date)

# Rename specific columns for consistency
colnames(patient_data_renamed)[colnames(patient_data_renamed) == "bp_diastolic_mmhg"] <- "bp_diastolic"
colnames(patient_data_renamed)[colnames(patient_data_renamed) == "bp_systolic_mmhg"] <- "bp_systolic"
colnames(visit_data_renamed)[colnames(visit_data_renamed) == "bmi_kg_m^2"] <- "bmi"

# Add suffixes to duplicate columns, excluding "patient_id"
common_cols <- intersect(colnames(patient_data_renamed), colnames(visit_data_renamed))
common_cols <- setdiff(common_cols, "patient_id")

# Add suffixes to common columns in both datasets
colnames(patient_data_renamed)[colnames(patient_data_renamed) %in% common_cols] <- paste0(common_cols, "_from_patient_dataset")
colnames(visit_data_renamed)[colnames(visit_data_renamed) %in% common_cols] <- paste0(common_cols, "_from_visit_dataset")

# Capture original and standarized names
capture_output_to_html(
  "data_renaming.html",
  "Data Renaming",
  "Patient Data Original Names" = colnames(patient_data_anonymized),
  "Patient Data Standarized Names" = colnames(patient_data_renamed),
  "Visit Data Original Names" = colnames(patient_data_renamed),
  "Visit Data Standarized Names" = colnames(patient_data_renamed)
  )
```

> Refer to [Appendix] - [Helper Functions] - [Function to Capture Output as HTML](#capture-output-to-html) for function definition.

```{r results='asis', echo=FALSE}
library(htmltools)

# Read the HTML file
html_content <- HTML(readLines("outputs/data_renaming.html"))

# Wrap the HTML content in a div with the 'scrollable-container' class
div_container <- div(class = "scrollable-container", html_content)

# Output the content
cat(as.character(div_container))
```

#### Type Standarization

Data types were converted to ensure compatibility with analytical processes. Dates, categorical variables, and numerical data were reformatted for accurate statistical analysis and modeling.

```{r conversions, eval=FALSE}
# Load required libraries
library(tidyr)
library(purrr)
library(lubridate)
  
# Helper functions for data type conversion
convert_to_date <- function(col) {
  parsed_dates <- parse_date_time(col, orders = c("ymd", "dmy", "mdy"), quiet = TRUE)
  ifelse(is.na(parsed_dates), NA, as.Date(parsed_dates))
}

convert_to_posix <- function(col) {
  if (is.numeric(col)) as.POSIXct(col, origin = "1970-01-01") else col
}

convert_to_list <- function(col) {
  if (is.character(col)) {
    str_split(col, pattern = "[,;\n|]+")
  } else {
    col
  }
}

safe_numeric <- function(col) {
  if (is.list(col)) {
    as.numeric(unlist(col))
  } else {
    suppressWarnings(as.numeric(col))
  }
}

# Function to convert data types and summarize changes
convert_data_types <- function(dataset, conversions, dataset_name) {
  initial_types <- map_chr(dataset, ~ paste(class(.x), collapse = ", "))
  
  dataset <- dataset %>%
    mutate(
      across(any_of(conversions$Date), convert_to_date),
      across(any_of(conversions$POSIX), convert_to_posix),
      across(any_of(conversions$Factor), as.factor),
      across(any_of(conversions$Numeric), safe_numeric),
      across(any_of(conversions$Logical), as.logical),
      across(any_of(conversions$List), convert_to_list)
    )
  
  final_types <- map_chr(dataset, ~ paste(class(.x), collapse = ", "))
  changes <- tibble(Column = names(dataset), 'Initial Type' = initial_types, 'Final Type' = final_types)
  
  list(
    dataset = dataset,
    conversion_summary = changes
  )
}

# Define type conversions for the patient dataset
patient_conversions <- list(
  Date = c(),
  Factor = c(
    "state", "marital_status", "gender_identity_from_patient_dataset", "sdoh_risk_level",
    "language_from_patient_dataset", "legal_sex_from_patient_dataset", "country_from_patient_dataset",
    "country_county_from_patient_dataset", "religion_from_patient_dataset",
    "sexual_orientation_from_patient_dataset", "sex_assigned_at_birth_from_patient_dataset",
    "level_of_service_from_patient_dataset", "mychart_status_from_patient_dataset",
    "patient_ethnic_group_from_patient_dataset", "patient_race_from_patient_dataset",
    "rural_urban_commuting_area_primary_from_patient_dataset", "rural_urban_commuting_area_secondary_from_patient_dataset",
    "patient_id_from_patient_dataset", "postal_code_from_patient_dataset", 
    "state_from_patient_dataset"
  ),
  Numeric = c(
    "adi_national_percentile", "adi_state_decile", "bmi_from_patient_dataset",
    "bp_diastolic_from_patient_dataset", "bp_systolic_from_patient_dataset", 
    "year_of_birth_from_patient_dataset", "svi_2020_socioeconomic_percentile_census_tract"
  ),
  Logical = c(
    "interpreter_needed_from_patient_dataset", "university_of_pennsylvania_student_from_patient_dataset"
  ),
  List = c(
    "allergies_and_contraindications", "chief_complaint", "hospital_or_clinic_administered_medications",
    "medical_history", "medications", "outpatient_medications", "procedures", "procedures_ordered_from_patient_dataset",
    "sdoh_domains", "phq_2_total_score", "phq_9", "medications_ordered_from_patient_dataset",
    "diagnosis_from_patient_dataset", "general_risk_score"
  )
)

# Define type conversions for the visit dataset
visit_conversions <- list(
  Date = c("visit_date", "appointment_creation_date"),
  POSIX = "appointment_time",
  Factor = c(
    "visit_type", "appointment_status", "encounter_type", "primary_benefit_plan",
    "primary_diagnosis", "primary_payer", "primary_payer_financial_class", 
    "primary_provider_title", "primary_provider_type", "scheduling_source", 
    "gender_identity_from_visit_dataset", "language_from_visit_dataset", 
    "legal_sex_from_visit_dataset", "religion_from_visit_dataset", 
    "sexual_orientation_from_visit_dataset", "state_from_visit_dataset", 
    "country_from_visit_dataset", "country_county_from_visit_dataset", 
    "primary_subscriber_group_number", "postal_code_from_visit_dataset",
    "patient_ethnic_group_from_visit_dataset", "patient_race_from_visit_dataset",
    "rural_urban_commuting_area_primary_from_visit_dataset", "rural_urban_commuting_area_secondary_from_visit_dataset",
    "patient_id_from_visit_dataset", 
    "mychart_status_from_visit_dataset",
    "sex_assigned_at_birth_from_visit_dataset", "level_of_service_from_visit_dataset"
  ),
  Numeric = c(
    "age_at_visit_years", "appointment_length_minutes", "lead_time_days", "continuity_of_care",
    "copay_collected", "copay_due", "prepayment_collected", "prepayment_due",
    "time_physician_spent_post_charting_minutes", "time_physician_spent_pre_charting_minutes",
    "time_waiting_for_physician_minutes", "time_with_physician_minutes", 
    "no_show_probability", "bmi_from_visit_dataset", "bp_diastolic_from_visit_dataset", 
    "bp_systolic_from_visit_dataset", "year_of_birth_from_visit_dataset", "encounter_to_close_day"
  ),
  Logical = c(
    "new_to_department_specialty", "new_to_facility", "new_to_provider", 
    "portal_active_at_scheduling", "self_pay", "university_of_pennsylvania_student_from_visit_dataset",
    "interpreter_needed_from_visit_dataset"
  ),
  List = c(
    "diagnosis_from_visit_dataset", "procedures_ordered_from_visit_dataset", "medications_ordered_from_visit_dataset"
  )
)

# Apply conversions to patient and visit datasets
patient_data_converted <- convert_data_types(patient_data_renamed, patient_conversions, "Patient Data")
visit_data_converted <- convert_data_types(visit_data_renamed, visit_conversions, "Visit Data")

# Create a single combined lookup dataframe for variable names and types
variable_type_lookup <- data.frame(
  Variable = c(names(patient_data_converted), names(visit_data_converted)),
  Type = c(
    map_chr(patient_data_converted, ~ paste(class(.), collapse = ", ")),
    map_chr(visit_data_converted, ~ paste(class(.), collapse = ", "))
  ),
  stringsAsFactors = FALSE
)

# Apply conversions
patient_conversion_results <- convert_data_types(patient_data_renamed, patient_conversions, "Patient Data")
patient_data_converted <- patient_conversion_results$dataset
visit_conversion_results <- convert_data_types(visit_data_renamed, visit_conversions, "Visit Data")
patient_data_converted <- patient_conversion_results$dataset
visit_data_converted <- visit_conversion_results$dataset

# Create a lookup dataframe for variable types
variable_type_lookup <- data.frame(
  Variable = c(names(patient_data_converted), names(visit_data_converted)),
  Type = c(
    map_chr(patient_data_converted, ~ paste(class(.), collapse = ", ")),
    map_chr(visit_data_converted, ~ paste(class(.), collapse = ", "))
  ),
  stringsAsFactors = FALSE
)

# Remove duplicates
variable_type_lookup <- variable_type_lookup[!duplicated(variable_type_lookup$Variable), ]

# Save lookup dataframe
saveRDS(variable_type_lookup, file = "datasets/variable_type_lookup.rds")

# Generate summaries
patient_conversion_summary <- patient_conversion_results$conversion_summary
visit_conversion_summary <- visit_conversion_results$conversion_summary

variable_type_lookup <- data.frame(
  Variable = c(names(patient_data_converted), names(visit_data_converted)),
  Type = c(
    map_chr(patient_data_converted, ~ paste(class(.), collapse = ", ")),
    map_chr(visit_data_converted, ~ paste(class(.), collapse = ", "))
  ),
  stringsAsFactors = FALSE
)
variable_type_lookup <- variable_type_lookup[!duplicated(variable_type_lookup$Variable), ]

# Capture conversion summaries to HTML
capture_output_to_html(
  "data_type_conversions.html",
  "Data Type Conversion Summary",
  "Patient Data" = patient_conversion_summary,
  "Visit Data" = visit_conversion_summary
)
```

> Refer to [Function to Capture Output as HTML](#capture-output-to-html) for function definition ([Appendix] - [Helper Functions]).

```{r results='asis', echo=FALSE}
library(htmltools)

# Read the HTML file
html_content <- HTML(readLines("outputs/data_type_conversions.html"))

# Wrap the HTML content in a div with the 'scrollable-container' class
div_container <- div(class = "scrollable-container", html_content)

# Output the content
cat(as.character(div_container))
```

#### Data Integration

The process involved merging patient and visit data based on unique identifiers, ensuring all related records were combined accurately.

##### Merging

Patient and visit data were merged using unique patient IDs. This process combined all related records into a unified dataset for comprehensive analysis. Any conflicts between overlapping columns were resolved by renaming non-identical columns with descriptive suffixes.

```{r merging, eval=FALSE}
# Load required libraries
library(dplyr)

# Verify unique patient_id values are shared
unique_patient_ids_patient_data <- unique(patient_data_converted$patient_id)
unique_patient_ids_visit_data <- unique(visit_data_converted$patient_id)

all_shared <- all(unique_patient_ids_patient_data %in% unique_patient_ids_visit_data) &&
              all(unique_patient_ids_visit_data %in% unique_patient_ids_patient_data)

if (all_shared) {
  # Perform the merge
  merged_data <- visit_data_converted %>%
  left_join(patient_data_converted, by = "patient_id")
  print("Merging complete.")
} else {
  print("There are patient_id values that are not shared between the two datasets.")
}
```

##### Deduplication

Duplicate columns were identified and processed. Identical values were consolidated, while non-identical columns were renamed appropriately to retain critical information.

```{r deduplication, eval=FALSE}
# Load required libraries
library(dplyr)

# Function to compare duplicate columns, check if they are identical, and remove duplicates
process_duplicate_columns <- function(data, suffix_1 = "_from_patient_dataset", suffix_2 = "_from_visit_dataset") {
  # Extract column names ending with the specified suffixes
  cols_suffix_1 <- grep(paste0(suffix_1, "$"), names(data), value = TRUE)
  cols_suffix_2 <- grep(paste0(suffix_2, "$"), names(data), value = TRUE)
  
  # Initialize an empty data frame for storing results
  results <- data.frame(Column_1 = character(), Column_2 = character(), Identical = logical(), stringsAsFactors = FALSE)
  
  # Vector to store non-identical column names
  non_identical_columns <- c()
  
  # Loop through columns with the same prefix but different suffixes
  for (col in cols_suffix_1) {
    # Derive the corresponding column name with the other suffix
    corresponding_col <- gsub(suffix_1, suffix_2, col)
    
    # Check if the corresponding column exists
    if (corresponding_col %in% names(data)) {
      identical_check <- identical(data[[col]], data[[corresponding_col]])
      results <- rbind(
        results,
        data.frame(
          Column_1 = col,
          Column_2 = corresponding_col,
          Identical = identical_check,
          stringsAsFactors = FALSE
        )
      )
      
      if (identical_check) {
        # Remove the suffix from the kept column
        new_name <- gsub(paste0("_from_patient_dataset|_from_visit_dataset"), "", col)
        names(data)[names(data) == col] <- new_name
        # Drop the duplicate column
        data <- data %>% select(-all_of(corresponding_col))
      } else {
        # Store non-identical columns for further analysis
        non_identical_columns <- c(non_identical_columns, col, corresponding_col)
      }
    } else {
      results <- rbind(
        results,
        data.frame(
          Column_1 = col,
          Column_2 = NA,
          Identical = NA,
          stringsAsFactors = FALSE
        )
      )
    }
  }
  
  # Return the updated data, the comparison results, and non-identical columns
  list(
    deduplicated_data = data,
    duplicate_check_results = results,
    non_identical_columns = unique(non_identical_columns)
  )
}

# Apply the function
processed_results <- process_duplicate_columns(merged_data)

# Extract the deduplicated dataset, duplicate check results, and non-identical columns
deduplicated_data <- processed_results$deduplicated_data
duplicate_column_check <- processed_results$duplicate_check_results
non_identical_columns <- processed_results$non_identical_columns

# Function to rename non-identical duplicate columns
rename_non_identical_columns <- function(data) {
  # Define mappings for non-identical columns
  rename_mapping <- c(
    "bmi_from_patient_dataset" = "patient_bmi",
    "bmi_from_visit_dataset" = "visit_bmi",
    "bp_diastolic_from_patient_dataset" = "patient_bp_diastolic",
    "bp_diastolic_from_visit_dataset" = "visit_bp_diastolic",
    "bp_systolic_from_patient_dataset" = "patient_bp_systolic",
    "bp_systolic_from_visit_dataset" = "visit_bp_systolic",
    "country_county_from_patient_dataset" = "patient_county",
    "country_county_from_visit_dataset" = "visit_county",
    "diagnosis_from_patient_dataset" = "patient_diagnosis",
    "diagnosis_from_visit_dataset" = "visit_diagnosis",
    "gender_identity_from_patient_dataset" = "patient_gender_identity",
    "gender_identity_from_visit_dataset" = "visit_gender_identity",
    "level_of_service_from_patient_dataset" = "patient_service_level",
    "level_of_service_from_visit_dataset" = "visit_service_level",
    "medications_ordered_from_patient_dataset" = "patient_medications_ordered",
    "medications_ordered_from_visit_dataset" = "visit_medications_ordered",
    "patient_race_from_patient_dataset" = "patient_race",
    "patient_race_from_visit_dataset" = "visit_race",
    "procedures_ordered_from_patient_dataset" = "patient_procedures_ordered",
    "procedures_ordered_from_visit_dataset" = "visit_procedures_ordered",
    "religion_from_patient_dataset" = "patient_religion",
    "religion_from_visit_dataset" = "visit_religion",
    "sex_assigned_at_birth_from_patient_dataset" = "patient_sex_assigned",
    "sex_assigned_at_birth_from_visit_dataset" = "visit_sex_assigned",
    "sexual_orientation_from_patient_dataset" = "patient_sexual_orientation",
    "sexual_orientation_from_visit_dataset" = "visit_sexual_orientation",
    "postal_code_from_patient_dataset" = "patient_postal_code",
    "postal_code_from_visit_dataset" = "visit_postal_code"
  )
  
  # Rename columns in the dataset based on the mapping
  renamed_data <- data %>%
    rename_with(~ ifelse(.x %in% names(rename_mapping), rename_mapping[.x], .x))
  
  return(renamed_data)
}

# Apply the function to the deduplicated dataset
deduplicated_data_renamed <- rename_non_identical_columns(deduplicated_data)
```

##### Reordering & Sorting

The merged dataset was reordered and sorted to prioritize relevant information. Columns were organized by their importance and relevance to the analysis objectives, and data was sorted primarily by patient ID and secondary by visit date. This organization facilitated easier access and manipulation of data for subsequent analyses.

```{r sorting, eval=FALSE}
# Load required libraries
library(dplyr)

# Reorder columns into logical groups
deduplicated_data_reordered <- deduplicated_data_renamed %>%
  select(
      # Identifiers
      patient_id,
      
      # Demographics
      age_at_visit_years, year_of_birth, patient_gender_identity, visit_gender_identity, patient_sex_assigned, visit_sex_assigned, language, interpreter_needed, legal_sex, marital_status, country, patient_county, visit_county, state, patient_postal_code, visit_postal_code,university_of_pennsylvania_student,
      
      # Visit Information
      visit_date, visit_type, appointment_creation_date, appointment_time, appointment_length_minutes, lead_time_days, continuity_of_care, portal_active_at_scheduling, new_to_department_specialty, new_to_facility, new_to_provider, encounter_type, scheduling_source,
      
      # Health Metrics
      patient_bmi, visit_bmi, patient_bp_diastolic, visit_bp_diastolic, patient_bp_systolic, visit_bp_systolic,
      
      # Symptoms, Diagnoses, and Procedures
      chief_complaint, primary_diagnosis, patient_diagnosis, visit_diagnosis, procedures, patient_procedures_ordered, visit_procedures_ordered,
      
      # Medications
      medications, outpatient_medications, patient_medications_ordered, visit_medications_ordered, hospital_or_clinic_administered_medications,
      
      # Service Level Information
      visit_service_level, patient_service_level,
      
      # Medical and Clinical History
      allergies_and_contraindications, medical_history,
      
      # Risk and Assessments
      general_risk_score, phq_2_total_score, phq_9, sdoh_domains, sdoh_risk_level,
      
      # Sociodemographic Context
      adi_national_percentile, adi_state_decile, rural_urban_commuting_area_primary, rural_urban_commuting_area_secondary, svi_2020_socioeconomic_percentile_census_tract,
      
      # Race, Ethnicity, Religion, and Sexual Orientation
      patient_race, visit_race, patient_religion, visit_religion, patient_ethnic_group, patient_sexual_orientation, visit_sexual_orientation,
      
      # Financial Information
      copay_collected, copay_due, prepayment_collected, prepayment_due, self_pay, primary_payer, primary_payer_financial_class, primary_subscriber_group_number, primary_benefit_plan,
      
      # Provider Information
      primary_provider_type, primary_provider_title,
      
      # Operational Metrics
    encounter_to_close_day, time_physician_spent_pre_charting_minutes, time_physician_spent_post_charting_minutes, time_waiting_for_physician_minutes, time_with_physician_minutes,
    
    # MyChart and Scheduling Metrics
    mychart_status, no_show_probability, appointment_status,

    # Remaining Columns (if any)
    everything()
  )

# Check reordered columns
print(names(deduplicated_data_reordered))

# Sort rows by patient_id and then by visit_date
data_preprocessed <- deduplicated_data_reordered %>%
  arrange(patient_id, visit_date)

# Identify variables in data_preprocessed that are not in the lookup table
new_variables <- setdiff(colnames(data_preprocessed), variable_type_lookup$Variable)

if (length(new_variables) > 0) {
  # Infer the types of the new variables
  inferred_types <- sapply(data_preprocessed[, new_variables, drop = FALSE], class)
  
  # Create a data frame for new variables and their inferred types
  new_rows <- data.frame(
    Variable = new_variables,
    Type = inferred_types,
    stringsAsFactors = FALSE
  )
  
  # Append the new variables with their types to the lookup table
  variable_type_lookup <- rbind(variable_type_lookup, new_rows)

# Save the updated lookup table
saveRDS(variable_type_lookup, "datasets/variable_type_lookup.rds")
}

# Capture Summary of Preprocessed Data
capture_output_to_html(
  "data_preprocessed_summary.html",
  "Preprocessed Data",
  "Summary Table" = variable_type_lookup %>%
    filter(Variable %in% colnames(data_preprocessed)) %>%
    arrange(match(Variable, colnames(data_preprocessed))) %>%
    rowwise() %>%
    mutate(
      Summary = calculate_summary_stats(
        variable_data = data_preprocessed[[Variable]],
        type = Type
      )$description
    ) %>%
    ungroup()
)
```

> Refer to [Appendix] - [Helper Functions] - [Function to Capture Output as HTML](#capture-output-to-html) for function definition.
>
> Refer to [Appendix] - [Helper Functions] - [Function to Calculate Summary Statistics](#calculate-summary-stats) for function definition.

```{r results='asis', echo=FALSE}
library(htmltools)

# Read the HTML file
html_content <- HTML(readLines("outputs/data_preprocessed_summary.html"))

# Wrap the HTML content in a div with the 'scrollable-container' class
div_container <- div(class = "scrollable-container", html_content)

# Output the content
cat(as.character(div_container))
```

### Missing Data Analysis

The analysis of missing data was performed to understand its extent and impact on the study. The process included visualizing missing data patterns and deciding on strategies to handle them. This was crucial to maintain the robustness of the statistical analysis and the reliability of the study results.

#### Missingness Visualization

Missing data patterns were analyzed to identify variables with significant gaps. Visualization techniques, such as bar plots, were used to summarize the extent of missingness.

```{r missing-plotting, eval=FALSE}
# Load required libraries
library(ggplot2)
library(dplyr)
library(tidyr)

# Visualize Missing Data
plot_missing_values_by_type(data_preprocessed, 0, 1)
plot_missing_values_by_type(data_preprocessed, 1, 5)
plot_missing_values_by_type(data_preprocessed, 5, 30)
plot_missing_values_by_type(data_preprocessed, 30, 50)
plot_missing_values_by_type(data_preprocessed, 50, 100)

# Capture Plots to HTML
capture_output_to_html(
  "missing_data_preprocessed_plots.html",
  "Missing Data Plots for data_preprocessed",
  "0-1% Missingness" = "../figures/plots/0-1_missing_data_preprocessed_plot.png",
  "1-5% Missingness" = "../figures/plots/1-5_missing_data_preprocessed_plot.png",
  "5-30% Missingness" = "../figures/plots/5-30_missing_data_preprocessed_plot.png",
  "30-50% Missingness" = "../figures/plots/30-50_missing_data_preprocessed_plot.png",
  "50-100% Missingness" = "../figures/plots/50-100_missing_data_preprocessed_plot.png"
)
```

> Refer to [Appendix] - [Helper Functions] - [Function to Plot Missing Values by Type](#plot-missing-values-by-type) for function definition.
>
> Refer to [Appendix] - [Helper Functions] - [Function to Capture Output as HTML](#capture-output-to-html) for function definition.

```{r results='asis', echo=FALSE}
library(htmltools)

# Read the HTML file
html_content <- HTML(readLines("outputs/missing_data_preprocessed_plots.html"))

# Wrap the HTML content in a div with the 'scrollable-container' class
div_container <- div(class = "scrollable-container", html_content)

# Output the content
cat(as.character(div_container))
```

#### Handling Strategies

Variables were categorized into levels of missingness (e.g., Low: 0–5%, Moderate: 5–30%) to guide data handling strategies. Techniques included:

-   **Imputation**: Using mean, median, or predictive modeling to fill missing values.

-   **Exclusion**: Removing variables or records with excessive missing data.

##### Default Strategies

Approaches such as imputation (mean, median, or predictive), categorical substitution, or exclusion were applied based on the extent and type of missing data.

```{r default-strategies, eval=FALSE}
# Define missing data handling options
missing_data_options <- list(
  "Low (0-5%)" = list(
    Numeric = c(
      "Predictive Modeling (Regression)",
      "Predictive Modeling (KNN)",
      "Multiple Imputation (MICE)",
      "Mean Imputation",
      "Median Imputation",
      "Linear Interpolation",
      "Spline Interpolation",
      "Indicator Variable + Mean",
      "Indicator Variable + Median"
    ),
    Factor = c(
      "Predictive Modeling (Decision Tree)",
      "Predictive Modeling (Random Forest)",
      "Frequency-Based Imputation (Weighted by Subgroup Frequency)",
      "Mode Imputation",
      "Add 'Unknown' Category",
      "Add 'Other' Category"
    ),
    Logical = c(
      "Predictive Modeling (Logistic Regression)",
      "Mode Imputation",
      "Add Indicator for Missingness",
      "Replace with Most Frequent Value",
      "Assume FALSE (if reasonable)",
      "Assume TRUE (if reasonable)"
    ),
    Date = c(
      "Median Date Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Use Most Frequent Date",
      "Use Previous Valid Value",
      "Use Subsequent Valid Value"
    ),
    POSIX = c(
      "Median Timestamp Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Use Most Frequent Timestamp",
      "Use Previous Valid Timestamp",
      "Use Subsequent Valid Timestamp"
    ),
    List = c(
      "Predictive Modeling (List Similarity-Based)",
      "Weighted Average List Imputation",
      "Replace with Most Frequent List",
      "Replace with Proxy List",
      "Replace with Empty List",
      "Add Indicator for Missingness"
    )
  ),
  "Moderate (5-30%)" = list(
    Numeric = c(
      "Multiple Imputation (MICE)",
      "Predictive Imputation (Regression)",
      "Predictive Imputation (KNN)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Mean Imputation",
      "Median Imputation",
      "Indicator Variable + Mean",
      "Indicator Variable + Median"
    ),
    Factor = c(
      "Predictive Modeling (Random Forest)",
      "Predictive Modeling (Decision Tree)",
      "Frequency-Based Imputation (Weighted by Subgroup Frequency)",
      "Add 'Unknown' Category",
      "Add 'Other' Category",
      "Mode Imputation"
    ),
    Logical = c(
      "Predictive Imputation (Logistic Regression)",
      "Add Indicator for Missingness",
      "Mode Imputation",
      "Replace with Most Frequent Value",
      "Assume FALSE",
      "Assume TRUE"
    ),
    Date = c(
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Indicator + Median Date Imputation",
      "Use Previous Valid Value",
      "Use Subsequent Valid Value"
    ),
    POSIX = c(
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Indicator + Median Timestamp Imputation",
      "Use Previous Valid Timestamp",
      "Use Subsequent Valid Timestamp"
    ),
    List = c(
      "Predictive Modeling (Clustering-Based List Imputation)",
      "Weighted Average List Imputation",
      "Replace with Most Frequent List",
      "Replace with Proxy List",
      "Replace with Empty List",
      "Create Synthetic List"
    )
  ),
  "High (30-50%)" = list(
    Numeric = c(
      "Indicator Variable + Multiple Imputation (MICE)",
      "Indicator Variable + Predictive Modeling (Regression)",
      "Indicator Variable + Predictive Modeling (KNN)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Mean Imputation",
      "Median Imputation"
    ),
    Factor = c(
      "Predictive Modeling (Random Forest)",
      "Predictive Modeling (Decision Tree)",
      "Add 'Unknown' Category",
      "Add 'Other' Category",
      "Frequency-Based Imputation (Weighted by Subgroup Frequency)",
      "Use Most Frequent Category"
    ),
    Logical = c(
      "Indicator Variable + Predictive Imputation",
      "Add Indicator for Missingness",
      "Mode Imputation",
      "Replace with Most Frequent Value",
      "Assume FALSE",
      "Assume TRUE"
    ),
    Date = c(
      "Indicator Variable + Median Date Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Use Placeholder Dates"
    ),
    POSIX = c(
      "Indicator Variable + Median Timestamp Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Use Placeholder Timestamps"
    ),
    List = c(
      "Add Indicator Variable for Missingness",
      "Replace with Most Frequent List",
      "Replace with Proxy List",
      "Replace with Empty List",
      "Create Synthetic List (via Sampling or Clustering)"
    )
  ),
  "Very High (>50%)" = list(
    Numeric = c(
      "Indicator Variable + Rough Imputation (e.g., Overall Mean)",
      "Indicator Variable + Multiple Imputation (MICE)",
      "Add 'Unknown' Category",
      "Drop Variable (if non-critical)"
    ),
    Factor = c(
      "Add 'Unknown' Category",
      "Add 'Other' Category",
      "Use Most Frequent Category",
      "Frequency-Based Imputation (Weighted by Subgroup Frequency)",
      "Drop Variable (if non-critical)"
    ),
    Logical = c(
      "Indicator Variable + Assume FALSE",
      "Replace with Most Frequent Value",
      "Assume TRUE",
      "Drop Variable (if non-critical)"
    ),
    Date = c(
      "Use Placeholder Dates",
      "Indicator Variable + Rough Date Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Drop Variable (if non-critical)"
    ),
    POSIX = c(
      "Use Placeholder Timestamps",
      "Indicator Variable + Rough Timestamp Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Drop Variable (if non-critical)"
    ),
    List = c(
      "Add Indicator for Missingness",
      "Replace with Empty List",
      "Replace with Proxy List",
      "Create Synthetic List",
      "Drop Variable (if non-critical)"
    )
  )
)
```

##### Strategy Selection

```{r missing-data-handling-table, eval=FALSE}
# Generate the data handling table
data_handling_table <- generate_data_handling_table(data_preprocessed)

capture_output_to_html(
  "data_preprocessed_handling_table.html",
  "Preprocessed Data Handling Table",
  "All Strategies" = data_handling_table
)
```

> Refer to [Appendix] - [Helper Functions] - [Function to Generate Missing Data Handling Strategies](#generate-data-handling-table) for function definition.
>
> Refer to [Appendix] - [Helper Functions] - [Function to Capture Output as HTML](#capture-output-to-html) for function definition.

```{r results='asis', echo=FALSE}
library(htmltools)

# Read the HTML file
html_content <- HTML(readLines("outputs/data_preprocessed_handling_table.html"))

# Wrap the HTML content in a div with the 'scrollable-container' class
div_container <- div(class = "scrollable-container", html_content)

# Output the content
cat(as.character(div_container))
```

### Data Reduction

A reduced dataset with fewer variables will be created to meet final project deadline. Later iterations will expand the analysis to the complete dataset.

```{r reduction, eval=FALSE}
# Load required libraries
library(dplyr)

# Reduce dataset
reduced_data_preprocessed <- data_preprocessed %>%
  filter(
    primary_provider_type %in% c("Physician", "Psychiatrist", "Resident", "Nurse Practitioner")
  ) %>%
  select(
    # Identifiers
    patient_id,
    
    # Demographics
    year_of_birth, legal_sex, marital_status, patient_ethnic_group, patient_race, adi_national_percentile,
    
    # Visit Information
    visit_date, appointment_length_minutes, appointment_status, primary_provider_type, visit_service_level,
    
    # Clinical Information
    allergies_and_contraindications, medical_history, primary_diagnosis, visit_diagnosis,

    # Medications
    medications, visit_medications_ordered,
    
    # Risk and Assessments
    general_risk_score, phq_2_total_score, phq_9, sdoh_risk_level,
  )
```

### Exploratory Data Analysis (EDA)

Exploratory Data Analysis will be conducted to gain insights into the main characteristics of the data, focusing on the distribution of key variables and the relationships between them. This will involve visualizing data distributions, identifying outliers, and exploring potential groupings and patterns. The findings from EDA will help in understanding the underlying structure of the data and guide the subsequent phases of data preprocessing and model building.

```{r eda, eval=FALSE}

```

### Data Transformation

Data will be transformed to fit the assumptions required for effective model training. This includes normalizing and scaling continuous variables to prevent attributes with larger ranges from dominating the model's feature importance. Categorical variables will be encoded into numerical values to facilitate their use in machine learning algorithms. Transformations such as logarithmic or square root transformations will be applied to skewed data to approximate normal distributions.

#### Normalization and Scaling

Numerical variables will be normalized using techniques such as Min-Max scaling or Z-score normalization to ensure equal contribution to models.

#### Encoding

Categorical variables will be converted into numerical representations using one-hot encoding or label encoding, depending on their nature.

### Feature Engineering

New features will be engineered to capture resource utilization effectively:

-   **Visit Count:** Total number of visits per patient.

-   **Visit Span:** Time span between a patient’s first and last recorded visits.

-   **Visit Density:** Number of visits divided by the duration of engagement.

-   **Diagnostic Complexity:** Count of unique diagnoses over time.

-   **Therapeutic Complexity:** Count of unique medications and procedures over time.

-   **Resource Utilization Score (RUS):** A composite metric combining visit density, diagnosis complexity, and medication burden to categorize patients into low, medium, or high utilization levels.

These features will be designed and normalized to improve model performance and enhance the interpretability of results.

#### Resource Utilization Score (RUS)

A Resource Utilization Score will be constructed using a composite index that integrates various dimensions of healthcare use, such as visit frequency, diversity of diagnoses, and medication load. This score will categorize patients into different levels of healthcare utilization, aiding in resource planning and management.

The RUS categorized patients into low, medium, or high utilization groups, providing insights into healthcare resource needs.

### Predictive Modeling

Predictive models will be developed to forecast healthcare resource utilization based on historical patient data. These models will help anticipate patient needs, optimizing resource allocation and scheduling. Both regression and classification models will be explored to predict continuous outcomes, such as the number of future visits, and categorical outcomes, such as high or low resource utilization.

#### Baseline Models

Baseline predictors, such as mean and median-based imputation, will serve as benchmarks for evaluating the performance of advanced algorithms.

#### Advanced Models

Both regression and classification models will be developed to predict resource utilization:

1.  **Regression Models:**

    -   **Linear Regression:** To obtain baseline performance for predicting continuous variables, such as visit frequency and duration.

    -   **Random Forest Regression:** To identify key predictors and achieve higher accuracy.

2.  **Classification Models:**

    -   **Logistic Regression:** Will categorize patients into low, medium, or high utilization groups.

    -   **Support Vector Machines (SVM):** To improve separation of utilization categories with complex patterns.

#### Evaluation Metrics

Model performance will be assessed using task-specific metrics:

-   **Regression:** Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared.

-   **Classification:** Accuracy, Precision, Recall, F1-score, and Area Under the Receiver Operating Characteristic Curve (AUC-ROC).

#### Validation and Optimization

To ensure generalizability, cross-validation techniques (e.g., k-fold cross-validation) will be employed. Grid search and random search methods will optimize hyperparameters for the best-performing models.

## Results

The results section will include:

1.  **Summary Statistics**

2.  **Model Performance**: Summary metrics highlighting predictive accuracy and reliability.

3.  **Feature Importance**: Insights into the most impactful predictors of resource utilization.

4.  **Validation**: Evaluation of model performance on unseen data.

## Conclusion {#sec-conclusion}

## Appendix

#### Helper Functions

##### Function to Capture Output as HTML {#capture-output-to-html}

```{r capture-to-html, eval=FALSE}
# Load required libraries
library(dplyr)

# Define function to capture output to HTML file
capture_output_to_html <- function(file, title, ...) {
    # Ensure file and title are provided
    if (missing(file)) stop("A file name must be provided.")
    if (missing(title)) stop("A title must be provided.")
    
    # Define the output path
    file <- file.path("outputs", file)
    dir.create(dirname(file), showWarnings = FALSE, recursive = TRUE)
    
    # Open the file for writing
    con <- file(file, "w")
    
    # Write HTML header
    cat("<html>\n<head>\n", file = con)
    cat(sprintf("<title>%s</title>\n", title), file = con)
    cat("<link rel=\"stylesheet\" href=\"../style.css\">\n", file = con)
    cat("</head>\n<body>\n", file = con)
    cat(sprintf("<h3>%s</h3>\n", title), file = con)
    
    # Iterate through sections
    args <- list(...)
    for (section_name in names(args)) {
        cat(sprintf("<h4>%s</h4>\n", section_name), file = con)
        
        if (is.data.frame(args[[section_name]])) {
            # Handle data frames
            cat("<table border='1' style='border-collapse:collapse; width:100%;'>\n", file = con)
            cat("<tr>", file = con)
            for (col_name in names(args[[section_name]])) {
                cat(sprintf("<th>%s</th>", col_name), file = con)
            }
            cat("</tr>\n", file = con)
            
            for (row in 1:nrow(args[[section_name]])) {
                cat("<tr>", file = con)
                for (col in names(args[[section_name]])) {
                    value <- as.character(args[[section_name]][row, col])
                    if (col == "summary_stats") value <- gsub("\n", "<br>", value)
                    cat(sprintf("<td>%s</td>", value), file = con)
                }
                cat("</tr>\n", file = con)
            }
            cat("</table>\n", file = con)
        } else if (is.character(args[[section_name]]) && all(grepl("\\.(png|jpg|jpeg|svg)$", args[[section_name]], ignore.case = TRUE))) {
            # Handle image paths
            for (img_path in args[[section_name]]) {
                cat(sprintf('<img src="%s" style="max-width:100%% height:auto">\n', img_path), file = con)
            }
        } else {
            # Handle other objects
            cat("<pre>\n", file = con)
            tryCatch(
                {
                    output <- capture.output(print(args[[section_name]]))
                    cat(paste(output, collapse = "\n"), "\n", file = con)
                },
                error = function(e) {
                    cat(sprintf("Error in section '%s': %s\n", section_name, e$message), file = con)
                }
            )
            cat("</pre>\n", file = con)
        }
    }
    
    # Close HTML tags and file connection
    cat("</body>\n</html>\n", file = con)
    close(con)
    message("HTML saved to: ", file)
}
```

##### Function to Calculate Summary Statistics {#calculate-summary-stats}

```{r calculate-summary-stats, eval=FALSE}
# Load required libraries
library(moments)

# Define function to calculate summary statistics 
calculate_summary_stats <- function(variable_data, type) {
  # Normalize type for cases like "hms, difftime"
  type <- strsplit(type, ",\\s*")[[1]][1]  # Take the first type if combined
  
  # Convert type to lowercase for consistent handling
  type <- tolower(type)
  
  if (all(is.na(variable_data))) {
    return(list(description = "All values are missing"))
  }
  
  if (type %in% c("numeric", "integer", "double")) {
    # Numeric summary
    stats <- summary(variable_data)
    sd_value <- sd(variable_data, na.rm = TRUE)
    skewness_value <- moments::skewness(variable_data, na.rm = TRUE)
    kurtosis_value <- moments::kurtosis(variable_data, na.rm = TRUE)
    range_values <- range(variable_data, na.rm = TRUE)
    
    description <- sprintf(
      "Range: %.1f to %.1f\n Median: %.1f\n Mean: %.1f\n SD: %.2f\n Skewness: %.2f\n Kurtosis: %.2f",
      range_values[1], range_values[2], stats["Median"], stats["Mean"], 
      sd_value, skewness_value, kurtosis_value
    )
    
  } else if (type == "factor") {
    # Factor summary
    levels_count <- if (!is.null(levels(variable_data))) nlevels(variable_data) else 0
    freq_table <- sort(table(variable_data), decreasing = TRUE)
    most_common <- if (length(freq_table) > 0) names(freq_table[1]) else "None"
    most_common_count <- if (length(freq_table) > 0) freq_table[1] else 0
    missing_levels <- sum(is.na(variable_data))
    
    description <- sprintf(
      "Levels: %d\n Most Common: %s (%d occurrences)\n Missing Levels: %d",
      levels_count, most_common, most_common_count, missing_levels
    )
    
  } else if (type == "logical") {
    # Logical summary
    true_percent <- if (length(variable_data[!is.na(variable_data)]) > 0) {
      mean(variable_data, na.rm = TRUE) * 100
    } else {
      0
    }
    false_percent <- 100 - true_percent
    
    description <- sprintf(
      "TRUE: %.1f%%\n FALSE: %.1f%%",
      true_percent, false_percent
    )
    
  } else if (type %in% c("date", "posixct", "hms", "difftime")) {
    # Date/Time summary
    range_values <- range(variable_data, na.rm = TRUE)
    missing_count <- sum(is.na(variable_data))
    
    description <- sprintf(
      "Range: %s to %s\n Missing: %d",
      format(range_values[1]), format(range_values[2]), missing_count
    )
    
  } else if (type == "list") {
    # List summary
    lengths <- sapply(variable_data, length)
    range_lengths <- range(lengths, na.rm = TRUE)
    median_length <- median(lengths, na.rm = TRUE)
    mean_length <- mean(lengths, na.rm = TRUE)
    
    description <- sprintf(
      "Range Lengths: %d to %d\n Median Length: %.1f\n Mean Length: %.1f",
      range_lengths[1], range_lengths[2], median_length, mean_length
    )
    
  } else if (type == "character") {
    # Character summary
    unique_values <- length(unique(variable_data[!is.na(variable_data)]))
    most_common <- if (length(variable_data[!is.na(variable_data)]) > 0) {
      names(sort(table(variable_data), decreasing = TRUE))[1]
    } else {
      "None"
    }
    missing_count <- sum(is.na(variable_data))
    
    description <- sprintf(
      "Unique Values: %d\n Most Common: %s\n Missing: %d",
      unique_values, most_common, missing_count
    )
    
  } else {
    # Unsupported type
    description <- sprintf("Unsupported type: %s", type)
  }
  
  return(list(description = description))
}
```

##### Function to Plot Missing Values by Type {#plot-missing-values-by-type}

```{r plot-missing-values, eval=FALSE}
# Load required libraries
library(ggplot2)
library(dplyr)
library(tidyr)

# Define function to plot missing values
plot_missing_values_by_type <- function(dataset, min_missing, max_missing, file_name = NULL) {
  # Get dataset name dynamically
  dataset_name <- deparse(substitute(dataset))
  
  # Automatically generate a default file name if none is provided
  if (is.null(file_name)) {
    file_name <- sprintf("figures/plots/%d-%d_missing_%s_plot.png", min_missing, max_missing, dataset_name)
  }
  
  # Create a dynamic title
  plot_title <- sprintf("Missing Values (%d-%d%%) for %s", min_missing, max_missing, dataset_name)
  
  # Ensure column types are matched with the lookup dataframe
  variable_types <- setNames(variable_type_lookup$Type, variable_type_lookup$Variable)
  
  # Create missing data summary
  missing_summary <- dataset %>%
    summarise(across(everything(), ~ sum(is.na(.)) / n() * 100)) %>%
    pivot_longer(everything(), names_to = "column", values_to = "missing_percentage") %>%
    mutate(type = variable_types[column]) %>%
    filter(missing_percentage > min_missing & missing_percentage <= max_missing)
  
  if (nrow(missing_summary) > 0) {
    threshold_inside <- max_missing * 0.8
    p <- ggplot(missing_summary, aes(x = reorder(column, -missing_percentage), y = missing_percentage, fill = type)) +
      geom_bar(stat = "identity", position = position_dodge2(width = 0.9)) +
      geom_text(data = missing_summary %>% filter(missing_percentage > 0), aes(
        label = ifelse(missing_percentage < 0.01, "<0.01", sprintf("%.2f", missing_percentage)),
        hjust = ifelse(missing_percentage > threshold_inside, 1.2, -0.2)
      ), size = 3, color = ifelse(missing_summary$missing_percentage > threshold_inside, "white", "black")) +
      coord_flip(clip = "off") +
      labs(
        title = plot_title, # Dynamic title
        x = "Column", 
        y = "Missing Percentage"
      ) +
      scale_fill_manual(values = c(
        "Date" = "#1f77b4", 
        "POSIXct" = "#ff7f0e", 
        "hms" = "#17becf",
        "factor" = "#2ca02c",
        "numeric" = "#d62728", 
        "logical" = "#9467bd", 
        "list" = "#8c564b"
      )) +
      theme_minimal() +
      theme(
        legend.position = "top", 
        legend.title = element_blank(), 
        legend.direction = "horizontal",
        axis.text.y = element_text(size = 7, hjust = 1),
        plot.title = element_text(size = 14, face = "bold", hjust = 0),
        plot.margin = margin(20, 20, 20, 20),
        panel.background = element_rect(fill = "white", color = NA), # White background
        plot.background = element_rect(fill = "white", color = NA)  # White background
      )
    
    # Save the plot to the file
    dir.create(dirname(file_name), showWarnings = FALSE, recursive = TRUE)
    ggsave(file_name, plot = p, width = 8, height = 6, bg = "white")
    message("Plot saved to: ", file_name)
  } else {
    message("No columns with missing data in the specified range.")
  }
}
```

##### Function to Generate Missing Data Handling Strategies {#generate-data-handling-table}

> Requires running [Function to Calculate Summary Statistics](#calculate-summary-stats) first.

```{r genetate-missing-data-handling-table, eval=FALSE}
# Define function to generate data handling table by variable types and missingness
generate_data_handling_table <- function(dataset, dataset_name = deparse(substitute(dataset))) {
  variable_type_lookup <- readRDS("datasets/variable_type_lookup.rds")
  variable_types <- setNames(variable_type_lookup$Type, variable_type_lookup$Variable)
  
  cat("Select function mode:\n")
  cat("1: Manually select handling strategy for each variable\n")
  cat("2: Print all handling strategy options for each variable\n")
  cat("3: Randomly select handling strategy for each variable (function testing)\n")
  
  repeat {
    selected_mode <- as.integer(readline("Enter your choice (1, 2, or 3): "))
    if (selected_mode %in% c(1, 2, 3)) break
    cat("Invalid choice. Try again.\n")
  }
  
  missing_data_summary <- dataset %>%
    summarise(across(everything(), ~ sum(is.na(.)) / n() * 100)) %>%
    pivot_longer(everything(), names_to = "variable", values_to = "missing_percentage") %>%
    filter(missing_percentage > 0) %>%
    rowwise() %>%
    mutate(
      type = case_when(
        grepl("numeric|integer|double", variable_types[variable], ignore.case = TRUE) ~ "Numeric",
        grepl("list", variable_types[variable], ignore.case = TRUE) ~ "List",
        grepl("factor", variable_types[variable], ignore.case = TRUE) ~ "Factor",
        grepl("logical", variable_types[variable], ignore.case = TRUE) ~ "Logical",
        grepl("character", variable_types[variable], ignore.case = TRUE) ~ "Character",
        grepl("date|posix|hms|difftime", variable_types[variable], ignore.case = TRUE) ~ "Date",
        TRUE ~ "Unsupported Type"
      ),
      summary_stats = gsub("\n", "<br>", calculate_summary_stats(dataset[[variable]], type)$description),
      missingness_level = case_when(
        missing_percentage <= 5 ~ "Low (0-5%)",
        missing_percentage > 5 & missing_percentage <= 30 ~ "Moderate (5-30%)",
        missing_percentage > 30 & missing_percentage <= 50 ~ "High (30-50%)",
        missing_percentage > 50 ~ "Very High (>50%)"
      )
    ) %>%
    ungroup()
  
  missing_data_summary <- missing_data_summary %>%
    rowwise() %>%
    mutate(
      handling_strategy = if (!is.null(missing_data_options[[missingness_level]][[type]])) {
        paste(
          sprintf("%d: %s", seq_along(missing_data_options[[missingness_level]][[type]]), 
                  missing_data_options[[missingness_level]][[type]]), 
          collapse = "<br>"
        )
      } else {
        "No valid options available"
      }
    ) %>%
    ungroup()
  
  # Sort and format data handling table
  data_handling_table <- missing_data_summary %>%
    mutate(
      missing_percentage_numeric = ifelse(missing_percentage < 0.1, 0, missing_percentage),
      miss = ifelse(
        missing_percentage < 0.1, "<0.1%", sprintf("%.1f%%", missing_percentage)
      )
    ) %>%
    arrange(missing_percentage_numeric) %>%
    select(variable, miss, summary_stats, handling_strategy)
  
  return(data_handling_table)
}
```

## Additional Visualizations
