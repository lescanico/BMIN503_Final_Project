---
title: "PURPOSe: Predicting Utilization of Resources in Psychiatry Outpatient Services"
subtitle: "BMIN503/EPID600 Final Project"
author: "Nicolas Lescano"
editor: visual
format:
  html:
    css: "style.css"
    self-contained: true
    embed-resources: true
    toc: true
    toc-depth: 5
    toc-location: left
    code-fold: true
    code-fold-default: true
    code-tools: true
execute:
  message: false
  warning: false
---

![](images/banner-removebg-preview.png){fig-align="left"}

------------------------------------------------------------------------

## Overview {#sec-overview}

This project leverages a decade of historical data from the Penn Behavioral Health Outpatient Psychiatry Clinic (PBH OPC) to develop predictive models aimed at optimizing resource utilization. The goal is to address operational inefficiencies in scheduling and care allocation, ultimately enhancing patient outcomes, clinic efficiency, and provider satisfaction. Integrating principles from psychiatry, healthcare operations, and data science, the project offers actionable insights into improving outpatient mental health care delivery.

The complete project repository, including scripts and datasets, is available [here](https://github.com/lescanico/BMIN503_Final_Project).

![](images/clipboard-2957942059.png)

## Introduction {#sec-introduction}

Outpatient psychiatry clinics often face significant challenges in managing the balance between patient demand and resource availability. Inefficiencies, such as long wait times, underutilized provider hours, and mismatches in the level of care, negatively impact patient outcomes and clinic operations. For instance, delayed follow-ups and uncoordinated care transitions contribute to treatment disruptions and patient dissatisfaction.

This project addresses these challenges by applying a data-driven approach to predict resource needs and inform scheduling decisions. Predictive models enable more accurate forecasting of appointment frequency, duration, and type of provider required, reducing operational inefficiencies and supporting proactive resource planning.

## Methods {#sec-methods}

### Data Sourcing & Anonymization

Patient and visit data spanning 10/1/2014 to 9/30/2024 were extracted from Epic Analytics platform. The raw files, originally in `.xlsx` format, were converted to `.csv` and securely stored as `patient_data.csv` and `visit_data.csv`.

Sensitive identifiers such as medical record numbers (MRNs) were replaced with anonymized IDs. Key demographics like birth dates and postal codes were generalized.

```{r anonymization, eval=FALSE}
# Load required libraries
library(readr)
library(dplyr)
library(lubridate)

# Import raw data
patient_data_raw <- read_csv("H:/secure/patient_data.csv")
visit_data_raw <- read_csv("H:/secure/visit_data.csv")

# Generate unique patient IDs for anonymization
mrn_mapping <- tibble(
  MRN = unique(patient_data_raw$MRN),
  patient_id = sprintf("%05d", seq_along(unique(patient_data_raw$MRN)))
)

# Save mrn_mapping for potential reversibility
mrn_mapping |> saveRDS("H:/secure/mrn_mapping.rds")

# Anonymize data by replacing sensitive identifiers
anonymize <- function(data, mapping) {
  data %>%
    left_join(mapping, by = "MRN") %>%
    mutate(
      year_of_birth = year(mdy(`Birth Date (UTC)`)),
      postal_code = substr(`Postal Code`, 1, 3)
    ) %>%
    select(-MRN, -`Birth Date (UTC)`, -`Postal Code`)
}

# Apply anonymization and save
patient_data_anonymized <- anonymize(patient_data_raw, mrn_mapping)
visit_data_anonymized <- anonymize(visit_data_raw, mrn_mapping)
patient_data_anonymized |> saveRDS("datasets/patient_data_anonymized.rds")
visit_data_anonymized |> saveRDS("datasets/visit_data_anonymized.rds")
```

### Data Preprocessing

#### Standarization

In the process of standardization, column names across datasets were reformatted to ensure consistency, clarity, and compatibility for downstream analysis. Special characters, spaces, and irregular naming conventions were replaced with concise, standardized formats, making the dataset easier to work with programmatically. To prevent conflicts during data integration, shared column names, excluding unique identifiers like `patient_id`, were appended with descriptive suffixes such as `_from_patient_dataset` and `_from_visit_dataset`. Additionally, redundant columns that were automatically generated during data export, such as `Start Date` and `End Date`, were identified and removed to streamline the data. A mapping table was created to document the original and standardized names, enabling traceability and facilitating efficient troubleshooting.

##### Variable Names

```{r renaming, eval=FALSE}
# Load required libraries
library(readr)
library(dplyr)
library(stringr)

# Load anonymized data
patient_data <- readRDS("datasets/patient_data_anonymized.rds")
visit_data <- readRDS("datasets/visit_data_anonymized.rds")

# Function to standardize column names
standardize_column_names <- function(dataset) {
  dataset |>
    rename_with(~ . |>
                  str_replace_all(" \\(mmHg\\)| \\(kg/m\\^2\\)", "") |>
                  str_to_lower() |>
                  str_replace_all("[\\s\\.\\/\\?\\-\\(\\)\\%\\$]+", "_") |>
                  str_replace_all("_+", "_") |>
                  str_replace_all("_$", ""))
}

# Function to create column name mapping
create_name_mapping <- function(original_dataset, standardized_dataset) {
  tibble(
    Original = colnames(original_dataset),
    Standardized = colnames(standardized_dataset)
  )
}

# Inspect column names
colnames(patient_data)
colnames(visit_data)

# Remove columns created automatically by Epic export process
removed_columns <- c('Start Date', 'End Date')

patient_data <- patient_data |> select(-any_of(removed_columns))
visit_data <- visit_data |> select(-any_of(removed_columns))

# Standardize column names
patient_data_renamed <- patient_data |> standardize_column_names()
visit_data_renamed <- visit_data |> standardize_column_names()

# Add suffixes to common columns, excluding "patient_id"
common_cols <- colnames(patient_data_renamed) |>
  intersect(colnames(visit_data_renamed)) |>
  setdiff("patient_id")

patient_data_renamed <- patient_data_renamed |> 
  rename_with(~ paste0(., "_from_patient_dataset"), .cols = common_cols)

visit_data_renamed <- visit_data_renamed |>
  rename_with(~ paste0(., "_from_visit_dataset"), .cols = common_cols)

# Create column name mappings
patient_name_mapping <- create_name_mapping(patient_data, patient_data_renamed) |>
  mutate(Source = "Patient Dataset")

visit_name_mapping <- create_name_mapping(visit_data, visit_data_renamed) |>
  mutate(Source = "Visit Dataset")

# Combine into a single dataframe
name_mapping <- bind_rows(patient_name_mapping, visit_name_mapping)

# Capture as HTML table
source("scripts/helper-functions/capture-to-html.R")

capture_output_to_html(
  "data_renaming.html",
  "Data Renaming Table" = name_mapping
)
```

> See [Function to Capture Output as HTML](#capture-output-as-html).

```{r, results='asis', echo=FALSE}
cat(readLines("outputs/data_renaming.html"), sep = "\n")
```

##### Variable Types

###### Classification

```{r classification, eval=FALSE}
# Load required libraries
library(tibble)
library(dplyr)
library(readr)

# Manually classify variable types
variable_type_mapping <- tibble(
  Variable = c(
    # Identifier
    "patient_id",
    
    # Patient Dataset - Numeric
    "adi_national_percentile", "adi_state_decile", "bmi_from_patient_dataset", "bp_diastolic_from_patient_dataset", "bp_systolic_from_patient_dataset", "general_risk_score", "svi_2020_socioeconomic_percentile_census_tract", "year_of_birth_from_patient_dataset",
    
    # Patient Dataset - List
    "allergies_and_contraindications", "chief_complaint", "diagnosis_from_patient_dataset", "hospital_or_clinic_administered_medications", "level_of_service_from_patient_dataset" , "medical_history", "medications", "medications_ordered_from_patient_dataset", "outpatient_medications", "phq_2_total_score", "phq_9", "procedures", "procedures_ordered_from_patient_dataset", "sdoh_domains", "sdoh_risk_level",
    
    # Patient Dataset - Factor
    "country_from_patient_dataset", "country_county_from_patient_dataset", "gender_identity_from_patient_dataset", "language_from_patient_dataset", "legal_sex_from_patient_dataset", "marital_status", "patient_ethnic_group_from_patient_dataset", "patient_race_from_patient_dataset", "religion_from_patient_dataset", "rural_urban_commuting_area_primary_from_patient_dataset", "rural_urban_commuting_area_secondary_from_patient_dataset", "sex_assigned_at_birth_from_patient_dataset", "sexual_orientation_from_patient_dataset", "state_from_patient_dataset", "postal_code_from_patient_dataset", "mychart_status_from_patient_dataset",
    
    # Patient Dataset - Logical
    "interpreter_needed_from_patient_dataset", "university_of_pennsylvania_student_from_patient_dataset",
    
    # Visit Dataset - Numeric
    "age_at_visit_years", "appointment_length_minutes", "bmi_from_visit_dataset", "bp_diastolic_from_visit_dataset", "bp_systolic_from_visit_dataset", "continuity_of_care", "copay_collected", "copay_due", "encounter_to_close_day", "lead_time_days", "no_show_probability", "prepayment_collected", "prepayment_due", "time_physician_spent_post_charting_minutes", "time_physician_spent_pre_charting_minutes", "time_waiting_for_physician_minutes", "time_with_physician_minutes", "year_of_birth_from_visit_dataset",
    
    # Visit Dataset - List
    "diagnosis_from_visit_dataset", "medications_ordered_from_visit_dataset", "procedures_ordered_from_visit_dataset",
    
    # Visit Dataset - Factor
    "appointment_status", "country_from_visit_dataset", "country_county_from_visit_dataset", "encounter_type", "gender_identity_from_visit_dataset", "language_from_visit_dataset", "legal_sex_from_visit_dataset", "level_of_service_from_visit_dataset", "patient_ethnic_group_from_visit_dataset", "patient_race_from_visit_dataset", "primary_benefit_plan", "primary_diagnosis", "primary_payer", "primary_payer_financial_class", "primary_provider_title", "primary_provider_type", "religion_from_visit_dataset", "rural_urban_commuting_area_primary_from_visit_dataset", "rural_urban_commuting_area_secondary_from_visit_dataset", "scheduling_source", "sex_assigned_at_birth_from_visit_dataset", "sexual_orientation_from_visit_dataset", "state_from_visit_dataset", "visit_type", "postal_code_from_visit_dataset", "primary_subscriber_group_number", "mychart_status_from_visit_dataset",
    
    # Visit Dataset - Logical
    "interpreter_needed_from_visit_dataset", "new_to_department_specialty", "new_to_facility", "new_to_provider", "portal_active_at_scheduling", "self_pay", "university_of_pennsylvania_student_from_visit_dataset",
    
    # Visit Dataset - Date
    "appointment_creation_date", "visit_date",
    
    # Visit Dataset - hms
    "appointment_time"
  ),
  
  Type = c(
    # Identifier
    rep("identifier", 1),
    # Patient Dataset - Numeric
    rep("numeric", 8),
    # Patient Dataset - List
    rep("list", 15),
    # Patient Dataset - Factor
    rep("factor", 16),
    # Patient Dataset - Logical
    rep("logical", 2),
    # Visit Dataset - Numeric
    rep("numeric", 18),
    # Visit Dataset - List
    rep("list", 3),
    # Visit Dataset - Factor
    rep("factor", 27),
    # Visit Dataset - Logical
    rep("logical", 7),
    # Visit Dataset - Date
    rep("Date", 2),
    # Visit Dataset - hms
    rep("hms", 1)
  )
)

# Group variables by type
group_summary <- variable_type_mapping |>
  group_by(Type) |>
  summarise(Variables = list(Variable), .groups = "drop") |>
  mutate(Count = lengths(Variables)) |>
  select(Type, Count, Variables)

# Save classification mapping
variable_type_mapping |> saveRDS("datasets/mappings/types.rds")
```

###### Conversion

```{r conversion, eval=FALSE}
# Load conversion function
source("scripts/helper-functions/convert-types.R")

# Apply function
patient_data_converted <- patient_data_renamed |> convert_types()
visit_data_converted <- visit_data_renamed |> convert_types()
```

> See [Function to Convert Data Types].

###### Mapping

```{r mapping, eval=FALSE}
# Load required libraries
library(tibble)
library(dplyr)

# Function to create variable type conversions table
create_type_conversions_mapping <- function(original_df, converted_df) {
  # Get data types for original and converted datasets
  tibble(
    Variable = names(original_df),
    Original_Type = sapply(original_df, function(x) paste(class(x), collapse = ", ")),
    Converted_Type = sapply(converted_df, function(x) paste(class(x), collapse = ", "))
  )
}

# Create the variable type mapping tables and combine them
type_conversions_mapping <- bind_rows(
  create_type_conversions_mapping(patient_data_renamed, patient_data_converted),
  create_type_conversions_mapping(visit_data_renamed, visit_data_converted)
)

# Capture as HTML table
source("scripts/helper-functions/capture-to-html.R")

capture_output_to_html(
  "data_type_conversions.html",
  "Data Type Conversions" = type_conversions_mapping
)
```

```{r, results='asis', echo=FALSE}
cat(readLines("outputs/data_type_conversions.html"), sep = "\n")
```

#### Cleaning

##### Missing Data Analysis

```{r plotting, eval=FALSE}
# Visualize Missing Data
source("scripts/helper-functions/plot-missing-values.R")

# Patient Data
plot_missing_values_by_type(patient_data_converted, 0, 5)
plot_missing_values_by_type(patient_data_converted, 5, 30)
plot_missing_values_by_type(patient_data_converted, 30, 50)
plot_missing_values_by_type(patient_data_converted, 50, 100)

# Visit Data
plot_missing_values_by_type(visit_data_converted, 0, 5)
plot_missing_values_by_type(visit_data_converted, 5, 30)
plot_missing_values_by_type(visit_data_converted, 30, 50)
plot_missing_values_by_type(visit_data_converted, 50, 100)

# Capture as HTML
source("scripts/helper-functions/capture-to-html.R")
capture_output_to_html(
  "patient_and_visit_missing_data_plots.html",
  "Missing Data Plots" = list(
    "figures/plots/0-5_missing_patient_data_converted_plot.png",
    "figures/plots/5-30_missing_patient_data_converted_plot.png",
    "figures/plots/30-50_missing_patient_data_converted_plot.png",
    "figures/plots/50-100_missing_patient_data_converted_plot.png",
    "figures/plots/0-5_missing_visit_data_converted_plot.png",
    "figures/plots/5-30_missing_visit_data_converted_plot.png",
    "figures/plots/30-50_missing_visit_data_converted_plot.png",
    "figures/plots/50-100_missing_visit_data_converted_plot.png"
    )
)
```

> See [Function to Plot Missing Values by Type].

```{r, results='asis', echo=FALSE}
cat(readLines("outputs/patient_and_visit_data_missingness_plots.html"), sep = "\n")
```

```{r handling, eval=FALSE}
# Generate data handling table with all options
source("scripts/helper-functions/generate-missing-handling-table.R")
data_handling_table <- generate_data_handling_table(data_organized)

# Capture as HTML
source("scripts/helper-functions/capture-to-html.R")
capture_output_to_html(
  "missing_data_handling_options.html",
  "Data Handling Table",
  "All Options" = data_handling_table
)
```

> See [Function to Generate Missing Data Handling Strategies](#generate-data-handling-table) and [Function to Calculate Summary Statistics](#calculate-summary-stats).

```{r, results='asis', echo=FALSE}
cat(readLines("outputs/missing_data_handling_options.html"), sep = "\n")
```

##### Merging

```{r merging, eval=FALSE}
# Load required libraries
library(dplyr)

# Verify unique patient_id values are shared
all_shared <- unique(patient_data_converted$patient_id) |>
  (\(x) all(x %in% visit_data_converted$patient_id))() &&
  unique(visit_data_converted$patient_id) |>
  (\(x) all(x %in% patient_data_converted$patient_id))()

if (all_shared) {
  # Perform the merge
  data_merged <- visit_data_converted |>
    left_join(patient_data_converted, by = "patient_id")
  
  print("Merging complete.")
} else {
  print("There are patient_id values that are not shared between the two datasets.")
}
```

##### Deduplication

```{r deduplication, eval=FALSE}
# Load required libraries
library(dplyr)
library(stringr)

# Function to process and rename only identical duplicate columns
process_identical_duplicates <- function(df, suffix_1 = "_from_patient_dataset", suffix_2 = "_from_visit_dataset") {
  # Identify columns with specified suffixes
  cols_suffix_1 <- grep(paste0(suffix_1, "$"), names(df), value = TRUE)
  cols_suffix_2 <- grep(paste0(suffix_2, "$"), names(df), value = TRUE)
  
  for (col in cols_suffix_1) {
    # Find corresponding column with the second suffix
    corresponding_col <- gsub(suffix_1, suffix_2, col)
    
    # Check if the corresponding column exists
    if (corresponding_col %in% cols_suffix_2) {
      # Check if the columns are identical
      if (identical(df[[col]], df[[corresponding_col]])) {
        # Consolidate identical columns by renaming the first and removing the second
        new_name <- gsub(paste0(suffix_1, "|", suffix_2), "", col)
        names(df)[names(df) == col] <- new_name
        df <- df %>% select(-all_of(corresponding_col))
      }
    }
  }
  
  # Return the deduplicated data
  return(df)
}

# Apply the function to the merged dataset
data_deduplicated <- process_identical_duplicates(data_merged)

# Update and save Variable Type Mapping
source("scripts/helper-functions/update-type-mapping.R")

updated_mapping <- variable_type_mapping |> update_type_mapping(data_deduplicated)
variable_type_mapping <- updated_mapping
variable_type_mapping |> saveRDS("datasets/mappings/types.rds")
```

> See [Function to Update Variable Type Mapping](#update-variable-type-mapping).

##### Organization

```{r organization, eval=FALSE}
# Load required libraries
library(readr)
library(dplyr)

logical_groups <- list(

  Identifiers = c("patient_id"),

  Demographic_Info = c(
    "year_of_birth", "age_at_visit_years", "legal_sex", 
    "gender_identity_from_patient_dataset", "gender_identity_from_visit_dataset", 
    "sex_assigned_at_birth_from_patient_dataset", "sex_assigned_at_birth_from_visit_dataset", 
    "sexual_orientation_from_patient_dataset", "sexual_orientation_from_visit_dataset", 
    "patient_race_from_patient_dataset", "patient_race_from_visit_dataset", 
    "patient_ethnic_group", "marital_status", "language", 
    "religion_from_patient_dataset", "religion_from_visit_dataset"
  ),
  
  Sociogeographic_Info = c(
    "country", "state", 
    "country_county_from_patient_dataset", "country_county_from_visit_dataset", 
    "postal_code_from_patient_dataset", "postal_code_from_visit_dataset", 
    "rural_urban_commuting_area_primary", "rural_urban_commuting_area_secondary", 
    "svi_2020_socioeconomic_percentile_census_tract", "adi_national_percentile", "adi_state_decile"
  ),
  
  Clinical_Info = c(
    "primary_diagnosis", "diagnosis_from_patient_dataset", "diagnosis_from_visit_dataset", 
    "medical_history", "allergies_and_contraindications"
  ),
  
  Treatment_Info = c(
    "procedures", "procedures_ordered_from_patient_dataset", "procedures_ordered_from_visit_dataset", 
    "medications", "hospital_or_clinic_administered_medications", "outpatient_medications", 
    "medications_ordered_from_patient_dataset", "medications_ordered_from_visit_dataset"
  ),
  
  Health_Metrics = c(
    "bmi_from_patient_dataset", "bmi_from_visit_dataset", 
    "bp_systolic_from_patient_dataset", "bp_systolic_from_visit_dataset", 
    "bp_diastolic_from_patient_dataset", "bp_diastolic_from_visit_dataset", 
    "phq_2_total_score", "phq_9", "general_risk_score", "sdoh_risk_level", "sdoh_domains"
  ),
  
  Encounter_Info = c(
    "visit_date", "visit_type", "continuity_of_care", "encounter_type", 
    "chief_complaint", "interpreter_needed", "appointment_status", 
    "appointment_creation_date", "appointment_time", "appointment_length_minutes", 
    "lead_time_days", "encounter_to_close_day", "scheduling_source", "no_show_probability"
  ),
  
  Provider_Info = c(
    "primary_provider_type", "primary_provider_title", 
    "time_physician_spent_pre_charting_minutes", "time_physician_spent_post_charting_minutes", 
    "time_with_physician_minutes", "time_waiting_for_physician_minutes"
  ),
  
  Financial_Info = c(
    "primary_benefit_plan", "primary_payer", "primary_payer_financial_class", "self_pay", 
    "copay_due", "copay_collected", "prepayment_due", "prepayment_collected", 
    "primary_subscriber_group_number", "level_of_service_from_patient_dataset", 
    "level_of_service_from_visit_dataset"
  ),
  
  Status_Info = c(
    "portal_active_at_scheduling", "mychart_status", 
    "new_to_department_specialty", "new_to_facility", "new_to_provider", 
    "university_of_pennsylvania_student"
  )
)

# Reorganize data
data_organized <- data_deduplicated |>
  select(all_of(unlist(logical_groups, use.names = FALSE))) |>
  arrange(patient_id, visit_date)

# Sample some rows
set.seed(123)
sample_data <- data_organized |>
  slice_sample(n = 5)

# Capture as HTML table
source("scripts/helper-functions/capture-to-html.R")

capture_output_to_html(
  "data_organized_sample.html",
  "Organized Data Sample" = sample_data
)
```

> See [Function to Update Variable Type Mapping](#update-variable-type-mapping)

```{r, results='asis', echo=FALSE}
cat(readLines("outputs/data_organized_sample.html"), sep = "\n")
```

##### Aggregation

```{r aggregation, eval=FALSE}
# Load required libraries
library(tidyverse)
library(purrr)
library(readr)

# Custom mode function
get_mode <- function(x) {
  if (length(x) == 0 || all(is.na(x))) return(NA)  # Handle empty or NA input
  uniq_vals <- unique(na.omit(x))
  uniq_vals[which.max(tabulate(match(x, uniq_vals)))]
}

# Flatten all list variables
data_flattened <- data_organized |> 
  mutate(across(where(is.list), ~ map_chr(., ~ paste(.x, collapse = ", "))))

# Row-wise aggregation
data_aggregated <- data_flattened |> 
  rowwise() |> 
  mutate(
    # Aggregate general_risk_score by mode
    general_risk_score_mode = if (is.list(general_risk_score)) 
      get_mode(unlist(general_risk_score)) else general_risk_score,
    
    # Aggregate phq_2_total_score and phq_9 by mean
    phq_2_total_score_mean = mean(as.numeric(unlist(strsplit(phq_2_total_score, ", "))), na.rm = TRUE),
    phq_9_mean = mean(as.numeric(unlist(strsplit(phq_9, ", "))), na.rm = TRUE),
    
    # Aggregate other list variables by count
    across(
      where(is.character) & !all_of(c("general_risk_score", "phq_2_total_score", "phq_9")), 
      ~ length(unlist(strsplit(., ", "))),
      .names = "count_{.col}"
    )
  ) |> 
  ungroup()

# Save aggregated data
data_aggregated |> saveRDS("datasets/processed/data_aggregated.rds")

# Update the type mapping
corrected_type_mapping <- variable_type_mapping |> 
  mutate(Type = ifelse(Type == "list", "numeric", Type))

# Save the updated type mapping
corrected_type_mapping |> saveRDS("datasets/mappings/post-aggregation-types.rds")

# Sample some rows
set.seed(123)
sample_data <- data_aggregated |>
  slice_sample(n = 5)

# Capture as HTML table
source("scripts/helper-functions/capture-to-html.R")

capture_output_to_html(
  "data_aggregated_sample.html",
  "Aggregated Data Sample" = sample_data
)
```

```{r, results='asis', echo=FALSE}
cat(readLines("outputs/data_aggregated_sample.html"), sep = "\n")
```

```{r}
# Load required libraries
library(dplyr)
library(caret)
library(readr)

# Step 1: Load the imputed dataset and variable type mappings
data_imputed <- readRDS("datasets/data_imputed.rds")
variable_type_mapping <- readRDS("datasets/mappings/types.rds")

# Step 2: Correct the types in the mapping
corrected_type_mapping <- variable_type_mapping |>
  mutate(Type = ifelse(Type == "list", "numeric", Type))

# Step 3: Preserve patient_id as an identifier
identifier_column <- "patient_id"
identifiers <- data_imputed |>
  select(all_of(identifier_column))

# Step 4: Exclude identifier from modeling data
modeling_data <- data_imputed |>
  select(-all_of(identifier_column))

# Step 5: Handle character variables (categorical encoding)
# Identify character variables
char_vars <- corrected_type_mapping |>
  filter(Type == "character") |>
  pull(Variable)

# Convert character variables to numeric (one-hot encoding)
if (length(char_vars) > 0) {
  dummy_model <- dummyVars(" ~ .", data = modeling_data[char_vars])
  char_encoded <- predict(dummy_model, newdata = modeling_data[char_vars]) |>
    as.data.frame()
  modeling_data <- cbind(modeling_data |>
                           select(-all_of(char_vars)), char_encoded)
}

# Step 6: Drop or convert any remaining list columns
# Identify columns of type list
list_vars <- corrected_type_mapping |>
  filter(Type == "numeric") |>  # Lists were reclassified as numeric
  pull(Variable)

# Drop or convert invalid list columns to numeric (if necessary)
list_vars_in_data <- intersect(list_vars, names(modeling_data))
if (length(list_vars_in_data) > 0) {
  modeling_data[list_vars_in_data] <- lapply(modeling_data[list_vars_in_data], function(col) {
    if (is.list(col)) {
      # Example: Replace with length of each list as a numeric proxy
      sapply(col, function(x) if (is.null(x)) 0 else length(x))
    } else {
      col
    }
  })
}

# Step 7: Normalize numeric variables
# Identify numeric variables that exist in the dataset
num_vars <- corrected_type_mapping |>
  filter(Type == "numeric") |>
  pull(Variable)

# Filter numeric variables to those present in the dataset
existing_num_vars <- intersect(num_vars, names(modeling_data))

# Scale numeric variables
if (length(existing_num_vars) > 0) {
  scaler <- preProcess(modeling_data[existing_num_vars], method = c("center", "scale"))
  modeling_data[existing_num_vars] <- predict(scaler, modeling_data[existing_num_vars])
}

# Step 8: Combine the identifiers back with the prepared data (if needed)
prepared_data <- cbind(identifiers, modeling_data)

# Save the prepared dataset for modeling
prepared_data |> saveRDS("datasets/processed/data_prepared.rds")

# Save final type mapping
updated_mapping <- corrected_type_mapping |>
  mutate(Final_Type = ifelse(Type == "character", "encoded_numeric", Type))
updated_mapping |> saveRDS("datasets/mappings/final_types.rds")

# Confirmations
cat("Dataset prepared for predictive analytics.\n")
cat("Saved to 'datasets/data_final_for_modeling_with_id.rds'.\n")
cat("Updated type mapping saved to 'datasets/mappings/final_types.rds'.\n")
```

##### Encoding

```{r encoding, eval=FALSE}
# Load required libraries
library(dplyr)
library(tibble)
library(lubridate)

# Helper function to check if a column is of class "hms"
is_hms_column <- function(x) inherits(x, "hms")

# Encode variables in aggregated_data while sparing 'patient_id'
encoding_mappings <- list()

# Save original column order
original_column_order <- names(aggregated_data)

# Process all columns except 'patient_id'
aggregated_data_encoded <- aggregated_data |>
  select(-patient_id) |>
  mutate(across(
    where(is.list),
    ~ {
      var_name <- cur_column()
      # Aggregate list into a numeric summary (e.g., count of unique items)
      list_counts <- sapply(., \(x) if (is.null(x) || all(is.na(x))) 0 else length(unique(na.omit(x))))
      encoding_mappings[[var_name]] <<- tibble(Value = "count_unique_items", Code = "numeric_summary")
      list_counts
    }
  )) |>
  mutate(across(
    where(is.factor),
    ~ {
      var_name <- cur_column()
      levels_cleaned <- levels(.)
      if (anyNA(levels_cleaned)) levels_cleaned <- c(levels_cleaned, "NA")
      mapping <- tibble(
        Level = levels_cleaned,
        Code = seq_along(levels_cleaned)
      )
      encoding_mappings[[var_name]] <<- mapping
      as.numeric(factor(., levels = levels_cleaned))
    }
  )) |>
  mutate(across(
    where(is.character),
    ~ {
      var_name <- cur_column()
      unique_vals <- unique(c(., "NA"))
      mapping <- tibble(
        Value = unique_vals,
        Code = seq_along(unique_vals)
      )
      encoding_mappings[[var_name]] <<- mapping
      as.numeric(factor(., levels = unique_vals))
    }
  )) |>
  mutate(across(
    where(is.logical),
    ~ {
      var_name <- cur_column()
      mapping <- tibble(
        Value = c(FALSE, TRUE, NA),
        Code = c(0, 1, -1)
      )
      encoding_mappings[[var_name]] <<- mapping
      ifelse(is.na(.), -1, as.numeric(.))
    }
  )) |>
  mutate(across(
    where(is.Date),
    ~ {
      min_date <- min(., na.rm = TRUE)
      if (is.infinite(min_date)) {
        mapping <- tibble(Value = NA, Code = NA)
        encoding_mappings[[cur_column()]] <<- mapping
        return(NA)
      }
      as.numeric(. - min_date)
    }
  )) |>
  mutate(across(
    where(is_hms_column),
    ~ as.numeric(.)
  ))

# Add 'patient_id' back to its original position
aggregated_data_encoded <- aggregated_data_encoded |>
  bind_cols(select(aggregated_data, patient_id)) |>
  select(all_of(original_column_order))  # Reorder columns to match the original dataset

# Save aggregated and encoded data
aggregated_data_encoded |> saveRDS("datasets/data_encoded.rds")

# Save the encoding mappings
dir.create("datasets/mappings", recursive = TRUE, showWarnings = FALSE)
encoding_mappings |> saveRDS("datasets/mappings/encoding.rds")

# Print confirmation
cat("Encoding complete.\nDataset saved to 'datasets/data_encoded.rds'.\nMappings saved to 'datasets/mappings/encoding.rds'.\n")

# Validate Dimensions
validation_passed <- 
  (aggregated_data_encoded |> nrow()) == (aggregated_data |> nrow()) &&
  (aggregated_data_encoded |> ncol()) == (aggregated_data |> ncol())

if (!validation_passed) {
  stop("Validation failed: dimensions do not match.")
} else {
  cat("Validation passed: dimensions match.\n")
}

# Sample some rows
set.seed(123)
sample_data <- aggregated_data_encoded |>
  slice_sample(n = 5)

# Capture Organized Data Sample
source("scripts/helper-functions/capture-to-html.R")

capture_output_to_html(
  "data_encoded_sample.html",
  "Sample",
  "Data Encoded" = sample_data
)
```

```{r, results='asis', echo=FALSE}
cat(readLines("outputs/data_encoded_sample.html"), sep = "\n")
```

##### Imputation

```{r imputation, eval=FALSE}
# Load required libraries
library(readr)
library(dplyr)
library(mice)
library(zoo)
library(hms)

# Step 1: Load encoded dataset and types
aggregated_data_encoded <- readRDS("datasets/data_encoded.rds")
variable_type_mapping <- readRDS("datasets/mappings/types.rds")

# Step 2: Validate that all variables in `aggregated_data_encoded` are mapped
missing_variables <- setdiff(names(aggregated_data_encoded), variable_type_mapping$Variable)
if (length(missing_variables) > 0) {
  stop("The following variables are missing in the type mapping: ", paste(missing_variables, collapse = ", "))
}

# Step 3: Imputation based on type mappings
impute_variable <- function(variable, var_type, var_name = NULL) {
  if (var_type == "numeric") {
    # Numeric: Use mean imputation for <10% missing; else use MICE
    if (mean(is.na(variable)) < 0.1) {
      return(ifelse(is.na(variable), mean(variable, na.rm = TRUE), variable))
    } else {
      temp_data <- data.frame(var = variable, dummy = rnorm(length(variable)))
      data_imputed <- mice(temp_data, m = 1)
      return(mice::complete(data_imputed)$var)
    }
  } else if (var_type == "factor") {
    # Factor: Use mode imputation
    mode_value <- names(sort(table(variable), decreasing = TRUE))[1]
    return(ifelse(is.na(variable), mode_value, variable))
  } else if (var_type == "logical") {
    # Logical: Use FALSE for missing
    return(ifelse(is.na(variable), FALSE, variable))
  } else if (var_type == "Date") {
    # Date: Use median date
    median_date <- as.Date(median(as.numeric(variable), na.rm = TRUE), origin = "1970-01-01")
    return(ifelse(is.na(variable), median_date, variable))
  } else if (var_type == "hms") {
    # hms: Use median time
    median_time <- hms::as_hms(median(as.numeric(variable), na.rm = TRUE))
    return(ifelse(is.na(variable), median_time, variable))
  } else if (var_type == "identifier") {
    # Identifier: Do not impute
    return(variable)
  } else if (var_type == "list") {
    # Handle phq variables differently
    if (var_name %in% c("phq_2_total_score", "phq_9")) {
      overall_mean <- mean(unlist(variable), na.rm = TRUE)
      return(ifelse(is.na(variable), overall_mean, variable))
    } else {
      # For other list variables, replace with 0 (count of unique values)
      return(ifelse(is.na(variable), 0, variable))
    }
  } else {
    stop("Unsupported variable type: ", var_type)
  }
}

# Apply imputation across all variables
data_imputed <- aggregated_data_encoded %>%
  mutate(across(
    everything(),
    ~ impute_variable(.x, 
      variable_type_mapping$Type[variable_type_mapping$Variable == cur_column()], 
      cur_column()
    )
  ))

# Step 4: Save imputed data
dir.create("datasets/imputed", recursive = TRUE, showWarnings = FALSE)
data_imputed |> saveRDS("datasets/data_imputed.rds")

# Print confirmation
cat("Imputation complete. Imputed dataset saved to 'datasets/data_imputed.rds'.\n")

# Check for missing values across the dataset
missing_summary <- colSums(is.na(data_imputed))

# Print the summary of missing values
if (all(missing_summary == 0)) {
  cat("No missing values detected in the imputed dataset.\n")
} else {
  cat("Missing values detected in the following variables:\n")
  print(missing_summary[missing_summary > 0])
}

# Sample some rows
set.seed(123)
sample_data <- data_imputed |>
  slice_sample(n = 10)

# Capture Organized Data Sample
source("scripts/helper-functions/capture-to-html.R")

capture_output_to_html(
  "data_imputed_sample.html",
  "Sample",
  "Imputed Data" = sample_data
)
```

```{r, results='asis', echo=FALSE}
cat(readLines("outputs/data_imputed_sample.html"), sep = "\n")
```

### Preparation

#### Feature Engineering

```{r feature_engineering, eval=FALSE}
# Load required libraries
library(dplyr)
library(tidyr)

# Define dataset boundaries and span
dataset_start <- as.Date("2014-10-01")
dataset_end <- as.Date("2024-09-30")

# Load data and mappings
prepared_data <- readRDS("datasets/data_final_for_modeling_with_id.rds")
encoding <- readRDS("datasets/mappings/encoding.rds")

# Ensure engineered features directory exists
dir.create("datasets/engineered_features", recursive = TRUE, showWarnings = FALSE)

# Filter for relevant provider types
relevant_providers <- c("Physician", "Psychiatrist", "Resident", "Nurse Practitioner")
relevant_provider_codes <- encoding$primary_provider_type |>
  filter(Level %in% relevant_providers) |>
  pull(Code)

# Adjust and Filter Data
prepared_data <- prepared_data |>
  filter(primary_provider_type %in% relevant_provider_codes) |>  # Filter relevant providers
  mutate(
    visit_date = as.Date(visit_date, origin = "1970-01-01"),  # Ensure `visit_date` is a Date
    appointment_length_minutes = ifelse(appointment_length_minutes < 0, NA, appointment_length_minutes)  # Replace negative values
  )

# Feature Extraction


# Feature Engineering
featured_data <- prepared_data |>
  group_by(patient_id) |>
  summarise(
    visit_count = n(),
    first_visit = min(visit_date, na.rm = TRUE),
    last_visit = max(visit_date, na.rm = TRUE),
    visit_span = 

    visit_date_min = min(visit_date, na.rm = TRUE),
    visit_date_max = max(visit_date, na.rm = TRUE),
    observed_span_days = pmax(as.numeric(visit_date_max - visit_date_min) + 1, 7),  # Minimum span threshold of 7 days
    visit_count = n(),
    average_visit_length = mean(appointment_length_minutes, na.rm = TRUE),
    visit_density = visit_count / observed_span_days,
    visit_density = pmin(visit_density, 1),  # Cap density at 1 visit per day
    weekly_load_minutes = pmax(visit_density * average_visit_length * 7, 0)  # Avoid negatives
  ) |>
  ungroup()

# Replace NA values with 0 for interpretability
weekly_load_data <- weekly_load_data |>
  mutate(across(everything(), ~ replace_na(.x, 0)))

# Save the updated dataset
weekly_load_data |> saveRDS("datasets/data_featured.rds")

# Print confirmation
cat("Feature engineering complete. Results saved to 'datasets/engineered_features/weekly_load_data.rds'.\n")
```

### Current progress

![](images/clipboard-2928763158.png)

## Results

```{r rf, eval=FALSE}
# Load required libraries
library(readr)
library(dplyr)
library(ranger)
library(ggplot2)

# Load the dataset
prepared_data <- readRDS("datasets/data_prepared.rds")

# Data preparation
rf_data <- prepared_data %>%
  select(-patient_id, -visit_date_min, -visit_date_max) %>%  # Remove unnecessary columns
  na.omit()  # Remove rows with missing values

# Convert character columns to factors
rf_data <- rf_data %>%
  mutate(across(where(is.character), as.factor))

# Split into training and testing sets
set.seed(123)  # For reproducibility
train_index <- sample(seq_len(nrow(rf_data)), size = 0.7 * nrow(rf_data))
train_data <- rf_data[train_index, ]
test_data <- rf_data[-train_index, ]

# Train the random forest model using ranger
rf_model <- ranger(
  formula = weekly_load_minutes ~ .,  # Predict weekly_load_minutes
  data = train_data,                  # Training data
  num.trees = 500,                    # Number of trees
  mtry = floor(sqrt(ncol(train_data) - 1)),  # Number of predictors sampled at each split
  importance = "impurity",            # Calculate variable importance
  classification = FALSE,             # Regression mode
  seed = 123                          # For reproducibility
)

# Print model summary
print(rf_model)

# Predict on the test data
predictions <- predict(rf_model, data = test_data)$predictions

# Calculate Mean Squared Error (MSE)
mse <- mean((predictions - test_data$weekly_load_minutes)^2)
cat("Mean Squared Error (MSE):", mse, "\n")

# Extract variable importance
importance <- as.data.frame(rf_model$variable.importance)
colnames(importance) <- c("Importance")
importance <- importance %>%
  arrange(desc(Importance))

# Print the top 10 most important variables
cat("Top 10 Important Variables:\n")
print(head(importance, 10))

# Plot variable importance
top_vars <- head(importance, 10)  # Select top 10 variables
ggplot(top_vars, aes(x = reorder(row.names(top_vars), Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Top 10 Important Variables",
    x = "Variable",
    y = "Importance"
  ) +
  theme_minimal()
```

```{r scatter, eval=FALSE}
# Load required libraries
library(ggplot2)
library(dplyr)

# Generate predictions on OOB samples from the ranger model
oob_predictions <- rf_model$predictions  # OOB predictions
actual_values <- train_data$weekly_load_minutes  # Actual values from the training data

# Create a data frame for plotting
oob_results <- data.frame(
  Actual = actual_values,
  Predicted = oob_predictions
)

# Calculate Mean Squared Error (MSE)
mse <- mean((oob_results$Actual - oob_results$Predicted)^2)

# Create the scatter plot
scatter_plot <- ggplot(oob_results, aes(x = Actual, y = Predicted)) +
  geom_point(alpha = 0.6, color = "blue") +  # Scatter plot points
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red", size = 1) +  # Perfect prediction line
  labs(
    title = "Predicted vs. Actual Weekly Load (Minutes)",
    subtitle = paste("Mean Squared Error (MSE):", round(mse, 5)),
    x = "Actual Weekly Load (Minutes)",
    y = "Predicted Weekly Load (Minutes)"
  ) +
  theme_minimal()

# Print the plot
print(scatter_plot)

# Save the scatter plot to the directory
ggsave(
  filename = "figures/plots/predicted_vs_actual_weekly_load.png",
  plot = scatter_plot,
  width = 8,   # Width in inches
  height = 6,  # Height in inches
  dpi = 300    # Resolution in dots per inch
)

# Print confirmation
cat("Scatter plot saved to 'figures/plots/predicted_vs_actual_weekly_load.png'\n")
```

![](figures/plots/predicted_vs_actual_weekly_load.png)

## Conclusion {#sec-conclusion}

## Appendix

### Helper Functions

#### Function to Capture Output as HTML {#capture-output-as-html}

``` r
{{< include scripts/helper-functions/capture-to-html.R >}}
```

#### Function to Convert Data Types

``` r
{{< include scripts/helper-functions/convert-types.R >}}
```

#### Function to Update Variable Type Mapping {#update-variable-type-mapping}

``` r
{{< include scripts/helper-functions/update-type-mapping.R >}}
```

#### Function to Plot Missing Values by Type {#plot-missing-values-by-type}

``` r
{{< include scripts/helper-functions/calculate-summary-stats.R >}}
```

#### Function to Calculate Summary Statistics {#calculate-summary-stats}

``` r
{{< include scripts/helper-functions/calculate-summary-stats.R >}}
```

#### Function to Generate Missing Data Handling Strategies {#generate-data-handling-table}

``` r
{{< include scripts/helper-functions/calculate-summary-stats.R >}}
```

### Additional Visualizations
