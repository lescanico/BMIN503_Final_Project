---
title: "A Data-Driven Approach to Predicting and Optimizing Outpatient Psychiatry Resource Utilization"
subtitle: "BMIN503/EPID600 Final Project"
author: "Nicolas Lescano"
editor: visual
format:
  html:
    css: "style.css"
    self-contained: true
    embed-resources: true
    toc: true
    toc-depth: 5
    toc-location: left
    code-fold: true
    code-fold-default: true
    code-tools: true
execute:
  message: false
  warning: false
---

![](images/banner.jpg){style="background-color: transparent" fig-align="left"}

------------------------------------------------------------------------

## Overview {#sec-overview}

This project leverages a decade of historical data from the Penn Behavioral Health Outpatient Psychiatry Clinic (PBH OPC) to develop predictive models aimed at optimizing resource utilization. The goal is to address operational inefficiencies in scheduling and care allocation, ultimately enhancing patient outcomes, clinic efficiency, and provider satisfaction. Integrating principles from psychiatry, healthcare operations, and data science, the project offers actionable insights into improving outpatient mental health care delivery.

The complete project repository, including scripts and datasets, is available [here](https://github.com/lescanico/BMIN503_Final_Project).

## Introduction {#sec-introduction}

Outpatient psychiatry clinics often face significant challenges in managing the balance between patient demand and resource availability. Inefficiencies, such as long wait times, underutilized provider hours, and mismatches in the level of care, negatively impact patient outcomes and clinic operations. For instance, delayed follow-ups and uncoordinated care transitions contribute to treatment disruptions and patient dissatisfaction.

This project addresses these challenges by applying a data-driven approach to predict resource needs and inform scheduling decisions. Predictive models enable more accurate forecasting of appointment frequency, duration, and type of provider required, reducing operational inefficiencies and supporting proactive resource planning.

## Methods {#sec-methods}

### Data Sourcing

Patient and visit data spanning 10/1/2014 to 9/30/2024 were extracted from Epic Analytics platform. The raw files, originally in `.xlsx` format, were converted to `.csv` and securely stored as `patient_data.csv` and `visit_data.csv`.

### Data Anonymization

Sensitive identifiers such as medical record numbers (MRNs) were replaced with anonymized IDs. Key demographics like birth dates and postal codes were generalized to ensure compliance with HIPAA and other privacy regulations.

```{r anonymization, eval=FALSE}
# Load required libraries
library(readr)
library(dplyr)
library(lubridate)

# Import raw data
patient_data_raw <- read_csv("H:/secure/patient_data.csv")
visit_data_raw <- read_csv("H:/secure/visit_data.csv")

# Generate unique patient IDs for anonymization
mrn_lookup <- tibble(
  MRN = unique(patient_data_raw$MRN),
  patient_id = sprintf("%05d", seq_along(unique(patient_data_raw$MRN)))
)

# Save mrn_lookup for potential reversibility
saveRDS(mrn_lookup, file = "H:/secure/mrn_lookup.rds")

# Anonymize data by replacing sensitive identifiers
anonymize <- function(data, lookup) {
  data %>%
    left_join(lookup, by = "MRN") %>%
    mutate(
      year_of_birth = year(mdy(`Birth Date (UTC)`)),
      postal_code = substr(`Postal Code`, 1, 3)
    ) %>%
    select(-MRN, -`Birth Date (UTC)`, -`Postal Code`)
}

# Apply anonymization and save
patient_data_anonymized <- anonymize(patient_data_raw, mrn_lookup)
visit_data_anonymized <- anonymize(visit_data_raw, mrn_lookup)
saveRDS(patient_data_anonymized, "datasets/patient_data_anonymized.rds")
saveRDS(visit_data_anonymized, "datasets/visit_data_anonymized.rds")
```

### Data Preprocessing

#### Name Standarization

Variable names were standardized (e.g., lowercase with underscores) for consistency. Duplicates arising from merged datasets were consolidated, with non-identical columns renamed using descriptive suffixes.

```{r renaming, eval=FALSE}
# Load required libraries
library(readr)
library(dplyr)
library(stringr)

# Load anonymized data
patient_data <- readRDS("datasets/patient_data_anonymized.rds")
visit_data <- readRDS("datasets/visit_data_anonymized.rds")

# Function to standardize column names
standardize_column_names <- function(dataset) {
  colnames(dataset) <- colnames(dataset) %>%
    str_replace_all(" \\(mmHg\\)| \\(kg/m\\^2\\)", "") %>% # Remove " (mmHg)" and " (kg/m^2)"
    str_to_lower() %>%                                   # Convert to lowercase
    str_replace_all("[\\s\\.\\/\\?\\-\\(\\)\\%\\$]+", "_") %>% # Replace special characters with underscores
    str_replace_all("_+", "_") %>%                       # Collapse multiple underscores
    str_replace_all("_$", "")                            # Remove trailing underscores
  dataset
}

# Function to create column name mapping
create_name_mapping <- function(original_dataset, standardized_dataset) {
  data.frame(
    Original = colnames(original_dataset),
    Standardized = colnames(standardized_dataset),
    stringsAsFactors = FALSE
  )
}

# Inspect column names
colnames(patient_data)
colnames(visit_data)

# Remove columns created automatically by Epic export process
removed_columns <- c('Start Date', 'End Date')
patient_data <- patient_data |> select(-any_of(removed_columns))
visit_data <- visit_data |> select(-any_of(removed_columns))

# Standardize column names
patient_data_renamed <- standardize_column_names(patient_data)
visit_data_renamed <- standardize_column_names(visit_data)

# Add suffixes to common columns, excluding "patient_id"
common_cols <- setdiff(intersect(colnames(patient_data_renamed), colnames(visit_data_renamed)), "patient_id")

patient_data_renamed <- patient_data_renamed %>% rename_with(~ paste0(., "_from_patient_dataset"), .cols = common_cols)
visit_data_renamed <- visit_data_renamed %>% rename_with(~ paste0(., "_from_visit_dataset"), .cols = common_cols)

# Create column name mappings
patient_name_mapping <- create_name_mapping(patient_data, patient_data_renamed)
patient_name_mapping$Source <- "Patient Dataset"

visit_name_mapping <- create_name_mapping(visit_data, visit_data_renamed)
visit_name_mapping$Source <- "Visit Dataset"

# Combine into a single dataframe
name_mapping <- bind_rows(patient_name_mapping, visit_name_mapping)

# Capture as HTML table
source("scripts/helper-functions/capture-to-html.R")

capture_output_to_html(
  "data_renaming.html",
  "Data Renaming Table",
  "Column Names" = name_mapping
)
```

```{r, results='asis', echo=FALSE}
source("scripts/helper-functions/embed-html-file.R")
embed_html("outputs/data_renaming.html")
```

#### Type Standarization

Data types were converted to ensure compatibility with analytical processes. Dates, categorical variables, and numerical data were reformatted for accurate statistical analysis and modeling.

```{r type-classification}
# Load required libraries
library(tibble)
library(dplyr)

# Define classifications for variables
type_classification <- tibble(
  Variable = c(
    
    # Identifier
    "patient_id",
    
    # Patient Dataset - Numeric
    "adi_national_percentile", "adi_state_decile", "bmi_from_patient_dataset",
    "bp_diastolic_from_patient_dataset", "bp_systolic_from_patient_dataset",
    "general_risk_score", "phq_2_total_score", "phq_9",
    "svi_2020_socioeconomic_percentile_census_tract", "year_of_birth_from_patient_dataset",
    
    # Patient Dataset - List
    "allergies_and_contraindications", "chief_complaint", "diagnosis_from_patient_dataset",
    "hospital_or_clinic_administered_medications", "medical_history", "medications",
    "medications_ordered_from_patient_dataset", "outpatient_medications", "procedures",
    "procedures_ordered_from_patient_dataset", "sdoh_domains",
    
    # Patient Dataset - Factor
    "country_from_patient_dataset", "country_county_from_patient_dataset",
    "gender_identity_from_patient_dataset", "language_from_patient_dataset",
    "legal_sex_from_patient_dataset", "level_of_service_from_patient_dataset",
    "marital_status", "patient_ethnic_group_from_patient_dataset",
    "patient_race_from_patient_dataset", "religion_from_patient_dataset",
    "rural_urban_commuting_area_primary_from_patient_dataset",
    "rural_urban_commuting_area_secondary_from_patient_dataset",
    "sdoh_risk_level", "sex_assigned_at_birth_from_patient_dataset",
    "sexual_orientation_from_patient_dataset", "state_from_patient_dataset", "postal_code_from_patient_dataset","mychart_status_from_patient_dataset",
    
    # Patient Dataset - Logical
    "interpreter_needed_from_patient_dataset", "university_of_pennsylvania_student_from_patient_dataset",
    
    # Visit Dataset - Numeric
    "age_at_visit_years", "appointment_length_minutes", "bmi_from_visit_dataset",
    "bp_diastolic_from_visit_dataset", "bp_systolic_from_visit_dataset",
    "continuity_of_care", "copay_collected", "copay_due", "encounter_to_close_day",
    "lead_time_days", "no_show_probability", "prepayment_collected", "prepayment_due",
    "time_physician_spent_post_charting_minutes", "time_physician_spent_pre_charting_minutes",
    "time_waiting_for_physician_minutes", "time_with_physician_minutes",
    "year_of_birth_from_visit_dataset",
    
    # Visit Dataset - List
    "diagnosis_from_visit_dataset", "medications_ordered_from_visit_dataset",
    "procedures_ordered_from_visit_dataset",
    
    # Visit Dataset - Factor
    "appointment_status", "country_from_visit_dataset",
    "country_county_from_visit_dataset", "encounter_type",
    "gender_identity_from_visit_dataset", "language_from_visit_dataset",
    "legal_sex_from_visit_dataset", "level_of_service_from_visit_dataset",
    "patient_ethnic_group_from_visit_dataset", "patient_race_from_visit_dataset",
    "primary_benefit_plan", "primary_diagnosis", "primary_payer",
    "primary_payer_financial_class", "primary_provider_title",
    "primary_provider_type", "religion_from_visit_dataset",
    "rural_urban_commuting_area_primary_from_visit_dataset",
    "rural_urban_commuting_area_secondary_from_visit_dataset",
    "scheduling_source", "sex_assigned_at_birth_from_visit_dataset",
    "sexual_orientation_from_visit_dataset", "state_from_visit_dataset",
    "visit_type", "postal_code_from_visit_dataset", "primary_subscriber_group_number", "mychart_status_from_visit_dataset",
    
    # Visit Dataset - Logical
    "interpreter_needed_from_visit_dataset",
    "new_to_department_specialty", "new_to_facility", "new_to_provider",
    "portal_active_at_scheduling", "self_pay",
    "university_of_pennsylvania_student_from_visit_dataset",
    
    # Visit Dataset - Date
    "appointment_creation_date", "visit_date",
    
    # Visit Dataset - hms
    "appointment_time"
  ),
  
  Type = c(
    # Identifier
    rep("identifier", 1),
    # Patient Dataset - Numeric
    rep("numeric", 10),
    # Patient Dataset - List
    rep("list", 11),
    # Patient Dataset - Factor
    rep("factor", 18),
    # Patient Dataset - Logical
    rep("logical", 2),
    # Visit Dataset - Numeric
    rep("numeric", 18),
    # Visit Dataset - List
    rep("list", 3),
    # Visit Dataset - Factor
    rep("factor", 27),
    # Visit Dataset - Logical
    rep("logical", 7),
    # Visit Dataset - Date
    rep("Date", 2),
    # Visit Dataset - hms
    rep("hms", 1)
  )
)

# Group variables by type
group_summary <- type_classification %>%
  group_by(Type) %>%
  summarise(Variables = list(Variable)) %>%
  mutate(Count = lengths(Variables)) %>%
  select(Type, Count, Variables)

# Capture as HTML table
source("scripts/helper-functions/capture-to-html.R")

capture_output_to_html(
  "variable_type_classification.html",
  "Variable Type Classification" = group_summary
)
```

```{r conversions, eval=FALSE}
# Load required libraries
library(dplyr)
library(lubridate)
library(hms)

# Function to convert data types
convert_types <- function(data) {
  for (i in seq_len(nrow(group_summary))) {
    vars <- group_summary$Variables[[i]]
    type <- group_summary$Type[i]
    
    # Only apply transformations to variables that exist in the dataset
    vars <- intersect(vars, names(data))
    if (length(vars) == 0) next  # Skip if no matching variables
    
    if (type == "numeric") {
      data <- data %>% mutate(across(any_of(vars), ~ suppressWarnings(as.numeric(as.character(.)))))
    } else if (type == "list") {
      data <- data %>% mutate(across(any_of(vars), ~ tryCatch(
        strsplit(iconv(as.character(.), from = "", to = "UTF-8"), ","), 
        error = function(e) { message("Error in list conversion: ", e); NA }
      )))
    } else if (type == "factor") {
      data <- data %>% mutate(across(any_of(vars), ~ as.factor(as.character(.))))
    } else if (type == "logical") {
      data <- data %>% mutate(across(any_of(vars), 
        ~ case_when(
          . == 1 ~ TRUE,
          . == 0 ~ FALSE,
          is.na(.) ~ NA,
          TRUE ~ NA
        )
      ))
    } else if (type == "Date") {
      data <- data %>% mutate(across(any_of(vars), 
        ~ tryCatch(
          lubridate::parse_date_time(., orders = c("ymd", "dmy", "mdy"), quiet = TRUE),
          error = function(e) { message("Error in Date conversion: ", e); NA }
        )
      ))
    } else if (type == "hms") {
      data <- data %>% mutate(across(any_of(vars), 
        ~ tryCatch(as_hms(.), 
                   error = function(e) { message("Error in hms conversion: ", e); NA })
      ))
    } else if (type == "identifier") {
      data <- data %>% mutate(across(any_of(vars), ~ as.character(.)))
    }
  }
  return(data)
}

# Apply the conversion function
patient_data_converted <- convert_types(patient_data_renamed)
visit_data_converted <- convert_types(visit_data_renamed)
```

```{r variable-type-lookup, eval=FALSE}
# Load required libraries
library(tibble)
library(dplyr)

# Function to create variable type conversions table
create_type_conversions_lookup<- function(original_data, converted_data) {
  # Get data types for original and converted datasets
  original_types <- sapply(original_data, function(x) paste(class(x), collapse = ", "))
  converted_types <- sapply(converted_data, function(x) paste(class(x), collapse = ", "))
  
  # Combine into a tibble for comparison
  type_conversions_lookup <- tibble(
    Variable = names(original_types),
    Original_Type = original_types,
    Converted_Type = converted_types
  )
  
  return(type_conversions_lookup)
}

# Create the variable type lookup table
patient_type_conversions_lookup <- create_type_conversions_lookup(patient_data_renamed, patient_data_converted)
visit_type_conversions_lookup <- create_type_conversions_lookup(visit_data_renamed, visit_data_converted)

type_conversions_lookup <- bind_rows(patient_type_conversions_lookup, visit_type_conversions_lookup)

# Save the lookup table to a file
saveRDS(type_conversions_lookup, "type_conversions_lookup.rds")

# Capture as HTML table
capture_output_to_html(
  "data_type_conversions.html",
  "Data Type Conversions" = type_conversions_lookup
)
```

```{r, results='asis', echo=FALSE}
embed_html("outputs/data_type_conversions.html")
```

#### Data Integration

The process involved merging patient and visit data based on unique identifiers, ensuring all related records were combined accurately.

##### Merging

Patient and visit data were merged using unique patient IDs. This process combined all related records into a unified dataset for comprehensive analysis. Any conflicts between overlapping columns were resolved by renaming non-identical columns with descriptive suffixes.

```{r merging, eval=FALSE}
# Load required libraries
library(dplyr)

# Verify unique patient_id values are shared
unique_patient_ids_patient_data <- unique(patient_data_converted$patient_id)
unique_patient_ids_visit_data <- unique(visit_data_converted$patient_id)

all_shared <- all(unique_patient_ids_patient_data %in% unique_patient_ids_visit_data) &&
              all(unique_patient_ids_visit_data %in% unique_patient_ids_patient_data)

if (all_shared) {
  # Perform the merge
  merged_data <- visit_data_converted %>%
  left_join(patient_data_converted, by = "patient_id")
  print("Merging complete.")
} else {
  print("There are patient_id values that are not shared between the two datasets.")
}
```

##### Deduplication

Duplicate columns were identified and processed. Identical values were consolidated, while non-identical columns were renamed appropriately to retain critical information.

```{r deduplication, eval=FALSE}
# Load required libraries
library(dplyr)

# Function to compare duplicate columns, check if they are identical, and remove duplicates
process_duplicate_columns <- function(data, suffix_1 = "_from_patient_dataset", suffix_2 = "_from_visit_dataset") {
  # Extract column names ending with the specified suffixes
  cols_suffix_1 <- grep(paste0(suffix_1, "$"), names(data), value = TRUE)
  cols_suffix_2 <- grep(paste0(suffix_2, "$"), names(data), value = TRUE)
  
  # Initialize an empty data frame for storing results
  results <- data.frame(Column_1 = character(), Column_2 = character(), Identical = logical(), stringsAsFactors = FALSE)
  
  # Vector to store non-identical column names
  non_identical_columns <- c()
  
  # Loop through columns with the same prefix but different suffixes
  for (col in cols_suffix_1) {
    # Derive the corresponding column name with the other suffix
    corresponding_col <- gsub(suffix_1, suffix_2, col)
    
    # Check if the corresponding column exists
    if (corresponding_col %in% names(data)) {
      identical_check <- identical(data[[col]], data[[corresponding_col]])
      results <- rbind(
        results,
        data.frame(
          Column_1 = col,
          Column_2 = corresponding_col,
          Identical = identical_check,
          stringsAsFactors = FALSE
        )
      )
      
      if (identical_check) {
        # Remove the suffix from the kept column
        new_name <- gsub(paste0("_from_patient_dataset|_from_visit_dataset"), "", col)
        names(data)[names(data) == col] <- new_name
        # Drop the duplicate column
        data <- data %>% select(-all_of(corresponding_col))
      } else {
        # Store non-identical columns for further analysis
        non_identical_columns <- c(non_identical_columns, col, corresponding_col)
      }
    } else {
      results <- rbind(
        results,
        data.frame(
          Column_1 = col,
          Column_2 = NA,
          Identical = NA,
          stringsAsFactors = FALSE
        )
      )
    }
  }
  
  # Return the updated data, the comparison results, and non-identical columns
  list(
    deduplicated_data = data,
    duplicate_check_results = results,
    non_identical_columns = unique(non_identical_columns)
  )
}

# Apply the function
processed_results <- process_duplicate_columns(merged_data)

# Extract the deduplicated dataset, duplicate check results, and non-identical columns
deduplicated_data <- processed_results$deduplicated_data
duplicate_column_check <- processed_results$duplicate_check_results
non_identical_columns <- processed_results$non_identical_columns



# Function to rename non-identical duplicate columns
rename_non_identical_columns <- function(data) {
  # Define mappings for non-identical columns
  rename_mapping <- c(
    "bmi_from_patient_dataset" = "patient_bmi",
    "bmi_from_visit_dataset" = "visit_bmi",
    "bp_diastolic_from_patient_dataset" = "patient_bp_diastolic",
    "bp_diastolic_from_visit_dataset" = "visit_bp_diastolic",
    "bp_systolic_from_patient_dataset" = "patient_bp_systolic",
    "bp_systolic_from_visit_dataset" = "visit_bp_systolic",
    "country_county_from_patient_dataset" = "patient_county",
    "country_county_from_visit_dataset" = "visit_county",
    "diagnosis_from_patient_dataset" = "patient_diagnosis",
    "diagnosis_from_visit_dataset" = "visit_diagnosis",
    "gender_identity_from_patient_dataset" = "patient_gender_identity",
    "gender_identity_from_visit_dataset" = "visit_gender_identity",
    "level_of_service_from_patient_dataset" = "patient_service_level",
    "level_of_service_from_visit_dataset" = "visit_service_level",
    "medications_ordered_from_patient_dataset" = "patient_medications_ordered",
    "medications_ordered_from_visit_dataset" = "visit_medications_ordered",
    "patient_race_from_patient_dataset" = "patient_race",
    "patient_race_from_visit_dataset" = "visit_race",
    "procedures_ordered_from_patient_dataset" = "patient_procedures_ordered",
    "procedures_ordered_from_visit_dataset" = "visit_procedures_ordered",
    "religion_from_patient_dataset" = "patient_religion",
    "religion_from_visit_dataset" = "visit_religion",
    "sex_assigned_at_birth_from_patient_dataset" = "patient_sex_assigned",
    "sex_assigned_at_birth_from_visit_dataset" = "visit_sex_assigned",
    "sexual_orientation_from_patient_dataset" = "patient_sexual_orientation",
    "sexual_orientation_from_visit_dataset" = "visit_sexual_orientation",
    "postal_code_from_patient_dataset" = "patient_postal_code",
    "postal_code_from_visit_dataset" = "visit_postal_code"
  )
  
  # Rename columns in the dataset based on the mapping
  renamed_data <- data %>%
    rename_with(~ ifelse(.x %in% names(rename_mapping), rename_mapping[.x], .x))
  
  return(renamed_data)
}

# Apply the function to the deduplicated dataset
deduplicated_data_renamed <- rename_non_identical_columns(deduplicated_data)
```

##### Reordering & Sorting

The merged dataset was reordered and sorted to prioritize relevant information. Columns were organized by their importance and relevance to the analysis objectives, and data was sorted primarily by patient ID and secondary by visit date. This organization facilitated easier access and manipulation of data for subsequent analyses.

```{r sorting, eval=FALSE}
# Load required libraries
library(dplyr)

# Reorder columns into logical groups
deduplicated_data_reordered <- deduplicated_data_renamed %>%
  select(
      # Identifiers
      patient_id,
      
      # Demographics
      year_of_birth, legal_sex, patient_gender_identity, visit_gender_identity, patient_sex_assigned, visit_sex_assigned, language, marital_status, country, patient_county, visit_county, state, patient_postal_code, visit_postal_code, university_of_pennsylvania_student,
      
      # Visit Information
      age_at_visit_years, visit_date, visit_type, appointment_creation_date, appointment_time, appointment_length_minutes, lead_time_days, continuity_of_care, portal_active_at_scheduling, new_to_department_specialty, new_to_facility, new_to_provider, encounter_type, scheduling_source,
      
      # Health Metrics
      patient_bmi, visit_bmi, patient_bp_diastolic, visit_bp_diastolic, patient_bp_systolic, visit_bp_systolic,
      
      # Symptoms, Diagnoses, and Procedures
      chief_complaint, primary_diagnosis, patient_diagnosis, visit_diagnosis, procedures, patient_procedures_ordered, visit_procedures_ordered,
      
      # Medications
      medications, outpatient_medications, patient_medications_ordered, visit_medications_ordered, hospital_or_clinic_administered_medications,
      
      # Service Level Information
      visit_service_level, patient_service_level,
      
      # Medical and Clinical History
      allergies_and_contraindications, medical_history,
      
      # Risk and Assessments
      general_risk_score, phq_2_total_score, phq_9, sdoh_domains, sdoh_risk_level,
      
      # Sociodemographic Context
      adi_national_percentile, adi_state_decile, rural_urban_commuting_area_primary, rural_urban_commuting_area_secondary, svi_2020_socioeconomic_percentile_census_tract,
      
      # Race, Ethnicity, Religion, and Sexual Orientation
      patient_race, visit_race, patient_religion, visit_religion, patient_ethnic_group, patient_sexual_orientation, visit_sexual_orientation,
      
      # Financial Information
      copay_collected, copay_due, prepayment_collected, prepayment_due, self_pay, primary_payer, primary_payer_financial_class, primary_subscriber_group_number, primary_benefit_plan,
      
      # Provider Information
      primary_provider_type, primary_provider_title,
      
      # Operational Metrics
    encounter_to_close_day, time_physician_spent_pre_charting_minutes, time_physician_spent_post_charting_minutes, time_waiting_for_physician_minutes, time_with_physician_minutes,
    
    # MyChart and Scheduling Metrics
    mychart_status, no_show_probability, appointment_status,

    # Remaining Columns (if any)
    everything()
  )

# Check reordered columns
print(names(deduplicated_data_reordered))

# Sort rows by patient_id and then by visit_date
data_preprocessed <- deduplicated_data_reordered %>%
  arrange(patient_id, visit_date)

# Identify variables in data_preprocessed that are not in the lookup table
new_variables <- setdiff(colnames(data_preprocessed), type_classification$Variable)

if (length(new_variables) > 0) {
  # Infer the types of the new variables
  inferred_types <- sapply(data_preprocessed[, new_variables, drop = FALSE], class)
  
  # Create a data frame for new variables
  new_rows <- data.frame(
    Variable = new_variables,
    Type = inferred_types
  )
  
  # Append the new variables with their types to the lookup table
  variable_type_lookup <- rbind(type_classification, new_rows)
  
  # Save the updated lookup table
  saveRDS(variable_type_lookup, "datasets/variable_type_lookup.rds")
}

# Inspect preprocessed data
head_data_preprocessed <- head(deduplicated_data_reordered)

# Sample some rows
set.seed(123)
sample_head <- data_preprocessed[sample(nrow(data_preprocessed), 10), ]

# Capture Summary of Preprocessed Data
source("scripts/helper-functions/capture-to-html.R")
capture_output_to_html(
  "data_preprocessed_head.html",
  "Preprocessed Data",
  "Few Sample Rows" = head(sample_head)
)
```

```{r, results='asis', echo=FALSE}
embed_html("outputs/data_preprocessed_head.html")
```

> Refer to [Appendix] - [Helper Functions] - [Function to Capture Output as HTML](#capture-output-to-html) for function definition.
>
> Refer to [Appendix] - [Helper Functions] - [Function to Calculate Summary Statistics](#calculate-summary-stats) for function definition.

### Missing Data Analysis

The analysis of missing data was performed to understand its extent and impact on the study. The process included visualizing missing data patterns and deciding on strategies to handle them. This was crucial to maintain the robustness of the statistical analysis and the reliability of the study results.

#### Missingness Visualization

Missing data patterns were analyzed to identify variables with significant gaps. Visualization techniques, such as bar plots, were used to summarize the extent of missingness.

```{r missing-plotting, eval=FALSE}
# Visualize Missing Data
source("scripts/helper-functions/plot-missing-values.R")

plot_missing_values_by_type(data_preprocessed, 0, 1)
plot_missing_values_by_type(data_preprocessed, 1, 5)
plot_missing_values_by_type(data_preprocessed, 5, 30)
plot_missing_values_by_type(data_preprocessed, 30, 50)
plot_missing_values_by_type(data_preprocessed, 50, 100)
```

![](figures/plots/0-1_missing_data_preprocessed_plot.png)

![](figures/plots/1-5_missing_data_preprocessed_plot.png)

![](figures/plots/5-30_missing_data_preprocessed_plot.png)

![](figures/plots/30-50_missing_data_preprocessed_plot.png)

![](figures/plots/50-100_missing_data_preprocessed_plot.png)

```{r, results='asis', echo=FALSE, eval=FALSE}
embed_html("outputs/missing_data_preprocessed_plots.html")
```

> Refer to [Appendix] - [Helper Functions] - [Function to Plot Missing Values by Type](#plot-missing-values-by-type) for function definition.
>
> Refer to [Appendix] - [Helper Functions] - [Function to Capture Output as HTML](#capture-output-to-html) for function definition.

#### Handling Strategies

Variables were categorized into levels of missingness (e.g., Low: 0–5%, Moderate: 5–30%) to guide data handling strategies. Techniques included:

-   **Imputation**: Using mean, median, or predictive modeling to fill missing values.

-   **Exclusion**: Removing variables or records with excessive missing data.

##### Default Strategies

Approaches such as imputation (mean, median, or predictive), categorical substitution, or exclusion were applied based on the extent and type of missing data.

```{r default-strategies, eval=FALSE}
# Define missing data handling options
missing_data_options <- list(
  "Low (0-5%)" = list(
    Numeric = c(
      "Predictive Modeling (Regression)",
      "Predictive Modeling (KNN)",
      "Multiple Imputation (MICE)",
      "Mean Imputation",
      "Median Imputation",
      "Linear Interpolation",
      "Spline Interpolation",
      "Indicator Variable + Mean",
      "Indicator Variable + Median"
    ),
    Factor = c(
      "Predictive Modeling (Decision Tree)",
      "Predictive Modeling (Random Forest)",
      "Frequency-Based Imputation (Weighted by Subgroup Frequency)",
      "Mode Imputation",
      "Add 'Unknown' Category",
      "Add 'Other' Category"
    ),
    Logical = c(
      "Predictive Modeling (Logistic Regression)",
      "Mode Imputation",
      "Add Indicator for Missingness",
      "Replace with Most Frequent Value",
      "Assume FALSE (if reasonable)",
      "Assume TRUE (if reasonable)"
    ),
    Date = c(
      "Median Date Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Use Most Frequent Date",
      "Use Previous Valid Value",
      "Use Subsequent Valid Value"
    ),
    hms = c(
      "Median Timestamp Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Use Most Frequent Timestamp",
      "Use Previous Valid Timestamp",
      "Use Subsequent Valid Timestamp"
    ),
    List = c(
      "Predictive Modeling (List Similarity-Based)",
      "Weighted Average List Imputation",
      "Replace with Most Frequent List",
      "Replace with Proxy List",
      "Replace with Empty List",
      "Add Indicator for Missingness"
    )
  ),
  "Moderate (5-30%)" = list(
    Numeric = c(
      "Multiple Imputation (MICE)",
      "Predictive Imputation (Regression)",
      "Predictive Imputation (KNN)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Mean Imputation",
      "Median Imputation",
      "Indicator Variable + Mean",
      "Indicator Variable + Median"
    ),
    Factor = c(
      "Predictive Modeling (Random Forest)",
      "Predictive Modeling (Decision Tree)",
      "Frequency-Based Imputation (Weighted by Subgroup Frequency)",
      "Add 'Unknown' Category",
      "Add 'Other' Category",
      "Mode Imputation"
    ),
    Logical = c(
      "Predictive Imputation (Logistic Regression)",
      "Add Indicator for Missingness",
      "Mode Imputation",
      "Replace with Most Frequent Value",
      "Assume FALSE",
      "Assume TRUE"
    ),
    Date = c(
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Indicator + Median Date Imputation",
      "Use Previous Valid Value",
      "Use Subsequent Valid Value"
    ),
    hms = c(
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Indicator + Median Timestamp Imputation",
      "Use Previous Valid Timestamp",
      "Use Subsequent Valid Timestamp"
    ),
    List = c(
      "Predictive Modeling (Clustering-Based List Imputation)",
      "Weighted Average List Imputation",
      "Replace with Most Frequent List",
      "Replace with Proxy List",
      "Replace with Empty List",
      "Create Synthetic List"
    )
  ),
  "High (30-50%)" = list(
    Numeric = c(
      "Indicator Variable + Multiple Imputation (MICE)",
      "Indicator Variable + Predictive Modeling (Regression)",
      "Indicator Variable + Predictive Modeling (KNN)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Mean Imputation",
      "Median Imputation"
    ),
    Factor = c(
      "Predictive Modeling (Random Forest)",
      "Predictive Modeling (Decision Tree)",
      "Add 'Unknown' Category",
      "Add 'Other' Category",
      "Frequency-Based Imputation (Weighted by Subgroup Frequency)",
      "Use Most Frequent Category"
    ),
    Logical = c(
      "Indicator Variable + Predictive Imputation",
      "Add Indicator for Missingness",
      "Mode Imputation",
      "Replace with Most Frequent Value",
      "Assume FALSE",
      "Assume TRUE"
    ),
    Date = c(
      "Indicator Variable + Median Date Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Use Placeholder Dates"
    ),
    hms = c(
      "Indicator Variable + Median Timestamp Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Linear Interpolation",
      "Spline Interpolation",
      "Use Placeholder Timestamps"
    ),
    List = c(
      "Add Indicator Variable for Missingness",
      "Replace with Most Frequent List",
      "Replace with Proxy List",
      "Replace with Empty List",
      "Create Synthetic List (via Sampling or Clustering)"
    )
  ),
  "Very High (>50%)" = list(
    Numeric = c(
      "Indicator Variable + Rough Imputation (e.g., Overall Mean)",
      "Indicator Variable + Multiple Imputation (MICE)",
      "Add 'Unknown' Category",
      "Drop Variable (if non-critical)"
    ),
    Factor = c(
      "Add 'Unknown' Category",
      "Add 'Other' Category",
      "Use Most Frequent Category",
      "Frequency-Based Imputation (Weighted by Subgroup Frequency)",
      "Drop Variable (if non-critical)"
    ),
    Logical = c(
      "Indicator Variable + Assume FALSE",
      "Replace with Most Frequent Value",
      "Assume TRUE",
      "Drop Variable (if non-critical)"
    ),
    Date = c(
      "Use Placeholder Dates",
      "Indicator Variable + Rough Date Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Drop Variable (if non-critical)"
    ),
    hms = c(
      "Use Placeholder Timestamps",
      "Indicator Variable + Rough Timestamp Imputation",
      "Forward Fill (if sequential)",
      "Backward Fill (if sequential)",
      "Drop Variable (if non-critical)"
    ),
    List = c(
      "Add Indicator for Missingness",
      "Replace with Empty List",
      "Replace with Proxy List",
      "Create Synthetic List",
      "Drop Variable (if non-critical)"
    )
  )
)
```

##### Strategy Selection

```{r missing-data-handling-table, eval=FALSE}
# Generate the data handling table
source("scripts/helper-functions/generate-missing-handling-table.R")
data_handling_table <- generate_data_handling_table(data_preprocessed)

capture_output_to_html(
  "data_preprocessed_handling_table_all_options.html",
  "Preprocessed Data Handling Table",
  "All Options" = data_handling_table
)

capture_output_to_html(
  "data_preprocessed_handling_table_manual.html",
  "Preprocessed Data Handling Table",
  "Manual" = data_handling_table
)

capture_output_to_html(
  "data_preprocessed_handling_table_random.html",
  "Preprocessed Data Handling Table",
  "Random" = data_handling_table
)
```

```{r, results='asis', echo=FALSE, eval=FALSE}
embed_html("outputs/data_preprocessed_handling_table_all_options.html")
```

> Refer to [Appendix] - [Helper Functions] - [Function to Generate Missing Data Handling Strategies](#generate-data-handling-table) for function definition
>
> Refer to [Appendix] - [Helper Functions] - [Function to Capture Output as HTML](#capture-output-to-html) for function definition.

### Data Reduction

A reduced dataset with fewer variables will be created to meet final project deadline. Later iterations will expand the analysis to the complete dataset.

```{r reduction, eval=FALSE}
# Load required libraries
library(dplyr)

# Reduce dataset
reduced_data_preprocessed <- data_preprocessed %>%
  filter(
    primary_provider_type %in% c("Physician", "Psychiatrist", "Resident", "Nurse Practitioner")
  ) %>%
  select(
    # Identifiers
    patient_id,
    
    # Demographics
    year_of_birth, legal_sex, marital_status, patient_ethnic_group, patient_race, adi_national_percentile,
    
    # Visit Information
    visit_date, appointment_length_minutes, appointment_status, primary_provider_type, visit_service_level,
    
    # Clinical Information
    allergies_and_contraindications, medical_history, primary_diagnosis, visit_diagnosis,

    # Medications
    medications, visit_medications_ordered,
    
    # Risk and Assessments
    general_risk_score, phq_2_total_score, phq_9, sdoh_risk_level,
  )
```

### Exploratory Data Analysis (EDA)

Exploratory Data Analysis will be conducted to gain insights into the main characteristics of the data, focusing on the distribution of key variables and the relationships between them. This will involve visualizing data distributions, identifying outliers, and exploring potential groupings and patterns. The findings from EDA will help in understanding the underlying structure of the data and guide the subsequent phases of data preprocessing and model building.

```{r eda, eval=FALSE}

```

### Data Transformation

Data will be transformed to fit the assumptions required for effective model training. This includes normalizing and scaling continuous variables to prevent attributes with larger ranges from dominating the model's feature importance. Categorical variables will be encoded into numerical values to facilitate their use in machine learning algorithms. Transformations such as logarithmic or square root transformations will be applied to skewed data to approximate normal distributions.

#### Normalization and Scaling

Numerical variables will be normalized using techniques such as Min-Max scaling or Z-score normalization to ensure equal contribution to models.

#### Encoding

Categorical variables will be converted into numerical representations using one-hot encoding or label encoding, depending on their nature.

### Feature Engineering

New features will be engineered to capture resource utilization effectively:

-   **Visit Count:** Total number of visits per patient.

-   **Visit Span:** Time span between a patient’s first and last recorded visits.

-   **Visit Density:** Number of visits divided by the duration of engagement.

-   **Diagnostic Complexity:** Count of unique diagnoses over time.

-   **Therapeutic Complexity:** Count of unique medications and procedures over time.

-   **Resource Utilization Score (RUS):** A composite metric combining visit density, diagnosis complexity, and medication burden to categorize patients into low, medium, or high utilization levels.

These features will be designed and normalized to improve model performance and enhance the interpretability of results.

#### Resource Utilization Score (RUS)

A Resource Utilization Score will be constructed using a composite index that integrates various dimensions of healthcare use, such as visit frequency, diversity of diagnoses, and medication load. This score will categorize patients into different levels of healthcare utilization, aiding in resource planning and management.

The RUS categorized patients into low, medium, or high utilization groups, providing insights into healthcare resource needs.

### Predictive Modeling

Predictive models will be developed to forecast healthcare resource utilization based on historical patient data. These models will help anticipate patient needs, optimizing resource allocation and scheduling. Both regression and classification models will be explored to predict continuous outcomes, such as the number of future visits, and categorical outcomes, such as high or low resource utilization.

#### Baseline Models

Baseline predictors, such as mean and median-based imputation, will serve as benchmarks for evaluating the performance of advanced algorithms.

#### Advanced Models

Both regression and classification models will be developed to predict resource utilization:

1.  **Regression Models:**

    -   **Linear Regression:** To obtain baseline performance for predicting continuous variables, such as visit frequency and duration.

    -   **Random Forest Regression:** To identify key predictors and achieve higher accuracy.

2.  **Classification Models:**

    -   **Logistic Regression:** Will categorize patients into low, medium, or high utilization groups.

    -   **Support Vector Machines (SVM):** To improve separation of utilization categories with complex patterns.

#### Evaluation Metrics

Model performance will be assessed using task-specific metrics:

-   **Regression:** Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared.

-   **Classification:** Accuracy, Precision, Recall, F1-score, and Area Under the Receiver Operating Characteristic Curve (AUC-ROC).

#### Validation and Optimization

To ensure generalizability, cross-validation techniques (e.g., k-fold cross-validation) will be employed. Grid search and random search methods will optimize hyperparameters for the best-performing models.

## Results

The results section will include:

1.  **Summary Statistics**

2.  **Model Performance**: Summary metrics highlighting predictive accuracy and reliability.

3.  **Feature Importance**: Insights into the most impactful predictors of resource utilization.

4.  **Validation**: Evaluation of model performance on unseen data.

## Conclusion {#sec-conclusion}

## Appendix

### Helper Functions

#### Function to Capture Output as HTML {#capture-output-as-html}

``` r
{{< include scripts/helper-functions/capture-to-html.R >}}
```

#### Function to Calculate Summary Statistics {#calculate-summary-stats}

``` r
{{< include scripts/helper-functions/calculate-summary-stats.R >}}
```

#### Function to Plot Missing Values by Type {#plot-missing-values-by-type}

``` r
{{< include scripts/helper-functions/calculate-summary-stats.R >}}
```

#### Function to Generate Missing Data Handling Strategies {#generate-data-handling-table}

``` r
{{< include scripts/helper-functions/calculate-summary-stats.R >}}
```

#### Function to Embed HTML files {#embed-html}

``` r
{{< include scripts/helper-functions/calculate-summary-stats.R >}}
```

#### Additional Visualizations
